<!DOCTYPE html>
<html lang="en">
<head hexo-theme='https://github.com/volantis-x/hexo-theme-volantis/tree/4.2.0'>
  <meta charset="utf-8">
  <!-- SEO相关 -->
  
    
  
  <!-- 渲染优化 -->
  <meta http-equiv='x-dns-prefetch-control' content='on' />
  <link rel='dns-prefetch' href='https://cdn.jsdelivr.net'>
  <link rel="preconnect" href="https://cdn.jsdelivr.net" crossorigin>
  <meta name="renderer" content="webkit">
  <meta name="force-rendering" content="webkit">
  <meta http-equiv="X-UA-Compatible" content="IE=Edge,chrome=1">
  <meta name="HandheldFriendly" content="True" >
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">

  <!-- 页面元数据 -->
  
  <title>恶意URL检测(九)一些比较杂的文章 - 虽然没什么用但还是想记下来</title>
  

  

  <!-- feed -->
  

  <!-- import meta -->
  

  <!-- link -->
  

  <!-- import link -->
  

  
    
<link rel="stylesheet" href="/css/first.css">

  

  
  <link rel="stylesheet" href="/css/style.css" media="print" onload="this.media='all';this.onload=null">
  <noscript><link rel="stylesheet" href="/css/style.css"></noscript>
  

  <script id="loadcss"></script>

</head>

<body>
  

<header id="l_header" class="l_header auto shadow blur show" style='opacity: 0' >
  <div class='container'>
  <div id='wrapper'>
    <div class='nav-sub'>
      <p class="title"></p>
      <ul class='switcher nav-list-h m-phone' id="pjax-header-nav-list">
        <li><a id="s-comment" class="fas fa-comments fa-fw" target="_self" href='javascript:void(0)'></a></li>
        
          <li><a id="s-toc" class="s-toc fas fa-list fa-fw" target="_self" href='javascript:void(0)'></a></li>
        
      </ul>
    </div>
		<div class="nav-main">
      
        
        <a class="title flat-box" target="_self" href='/'>
          
            <img no-lazy class='logo' src='https://cdn.jsdelivr.net/gh/volantis-x/cdn-org/blog/Logo-NavBar@3x.png'/>
          
          
          
        </a>
      

			<div class='menu navigation'>
				<ul class='nav-list-h m-pc'>
          
          
          
            
            
              <li>
                <a class="menuitem flat-box faa-parent animated-hover" href=/
                  
                  
                  
                    id="home"
                  >
                  <i class='fas fa-rss fa-fw'></i>博客
                </a>
                
              </li>
            
          
          
            
            
              <li>
                <a class="menuitem flat-box faa-parent animated-hover" href=/categories/
                  
                  
                  
                    id="categories"
                  >
                  <i class='fas fa-folder-open fa-fw'></i>分类
                </a>
                
              </li>
            
          
          
            
            
              <li>
                <a class="menuitem flat-box faa-parent animated-hover" href=/tags/
                  
                  
                  
                    id="tags"
                  >
                  <i class='fas fa-tags fa-fw'></i>标签
                </a>
                
              </li>
            
          
          
            
            
              <li>
                <a class="menuitem flat-box faa-parent animated-hover" href=/archives/
                  
                  
                  
                    id="archives"
                  >
                  <i class='fas fa-archive fa-fw'></i>归档
                </a>
                
              </li>
            
          
          
            
            
              <li>
                <a class="menuitem flat-box faa-parent animated-hover" href=/friends/
                  
                  
                  
                    id="friends"
                  >
                  <i class='fas fa-link fa-fw'></i>友链
                </a>
                
              </li>
            
          
          
            
            
              <li>
                <a class="menuitem flat-box faa-parent animated-hover" href=/about/
                  
                  
                  
                    id="about"
                  >
                  <i class='fas fa-info-circle fa-fw'></i>关于
                </a>
                
              </li>
            
          
          
				</ul>
			</div>

      <div class="m_search">
        <form name="searchform" class="form u-search-form">
          <i class="icon fas fa-search fa-fw"></i>
          <input type="text" class="input u-search-input" placeholder="Search..." />
        </form>
      </div>

			<ul class='switcher nav-list-h m-phone'>
				
					<li><a class="s-search fas fa-search fa-fw" target="_self" href='javascript:void(0)'></a></li>
				
				<li>
          <a class="s-menu fas fa-bars fa-fw" target="_self" href='javascript:void(0)'></a>
          <ul class="menu-phone list-v navigation white-box">
            
              
            
              <li>
                <a class="menuitem flat-box faa-parent animated-hover" href=/
                  
                  
                  
                    id="home"
                  >
                  <i class='fas fa-rss fa-fw'></i>博客
                </a>
                
              </li>
            
          
            
              
            
              <li>
                <a class="menuitem flat-box faa-parent animated-hover" href=/categories/
                  
                  
                  
                    id="categories"
                  >
                  <i class='fas fa-folder-open fa-fw'></i>分类
                </a>
                
              </li>
            
          
            
              
            
              <li>
                <a class="menuitem flat-box faa-parent animated-hover" href=/tags/
                  
                  
                  
                    id="tags"
                  >
                  <i class='fas fa-tags fa-fw'></i>标签
                </a>
                
              </li>
            
          
            
              
            
              <li>
                <a class="menuitem flat-box faa-parent animated-hover" href=/archives/
                  
                  
                  
                    id="archives"
                  >
                  <i class='fas fa-archive fa-fw'></i>归档
                </a>
                
              </li>
            
          
            
              
            
              <li>
                <a class="menuitem flat-box faa-parent animated-hover" href=/friends/
                  
                  
                  
                    id="friends"
                  >
                  <i class='fas fa-link fa-fw'></i>友链
                </a>
                
              </li>
            
          
            
              
            
              <li>
                <a class="menuitem flat-box faa-parent animated-hover" href=/about/
                  
                  
                  
                    id="about"
                  >
                  <i class='fas fa-info-circle fa-fw'></i>关于
                </a>
                
              </li>
            
          
            
          </ul>
        </li>
			</ul>
		</div>
	</div>
  </div>
</header>

  <div id="l_body">
    <div id="l_cover">
  
    
        <div id="full" class='cover-wrapper post dock' style="display: none;">
          
            <div class='cover-bg lazyload placeholder' data-bg="https://cdn.jsdelivr.net/gh/volantis-x/cdn-wallpaper-minimalist/2020/033.jpg"></div>
          
          <div class='cover-body'>
  <div class='top'>
    
    
      <p class="title">1ss4k</p>
    
    
  </div>
  <div class='bottom'>
    <div class='menu navigation'>
      <div class='list-h'>
        
          
            <a href="/v4/getting-started/"
              
              
              id="v4getting-started">
              <img src='https://cdn.jsdelivr.net/gh/twitter/twemoji@13.0/assets/svg/1f5c3.svg'><p>文档</p>
            </a>
          
            <a href="/faqs/"
              
              
              id="faqs">
              <img src='https://cdn.jsdelivr.net/gh/twitter/twemoji@13.0/assets/svg/1f516.svg'><p>帮助</p>
            </a>
          
            <a href="/examples/"
              
              
              id="examples">
              <img src='https://cdn.jsdelivr.net/gh/twitter/twemoji@13.0/assets/svg/1f396.svg'><p>示例</p>
            </a>
          
            <a href="/contributors/"
              
              
              id="contributors">
              <img src='https://cdn.jsdelivr.net/gh/twitter/twemoji@13.0/assets/svg/1f389.svg'><p>社区</p>
            </a>
          
            <a href="/archives/"
              
              
              id="archives">
              <img src='https://cdn.jsdelivr.net/gh/twitter/twemoji@13.0/assets/svg/1f4f0.svg'><p>博客</p>
            </a>
          
        
      </div>
    </div>
  </div>
</div>

          <div id="scroll-down" style="display: none;"><i class="fa fa-chevron-down scroll-down-effects"></i></div>
        </div>
    
  
  </div>

    <div id='safearea'>
      <div class='body-wrapper' id="pjax-container">
        

<div class='l_main'>
  <article class="article post white-box reveal md shadow article-type-post" id="post" itemscope itemprop="blogPost">
  


  
  <div class="article-meta" id="top">
    
    
    
      <h1 class="title">
        恶意URL检测(九)一些比较杂的文章
      </h1>
      <div class='new-meta-box'>
        
          
            
<div class='new-meta-item author'>
  <a class='author' href="/" rel="nofollow">
    <img no-lazy src="">
    <p>请设置文章作者</p>
  </a>
</div>

          
        
          
            
  <div class='new-meta-item category'>
    <a class='notlink'>
      <i class="fas fa-folder-open fa-fw" aria-hidden="true"></i>
      <a class="category-link" href="/categories/web/">web</a>
    </a>
  </div>


          
        
          
            <div class="new-meta-item date">
  <a class='notlink'>
    <i class="fas fa-calendar-alt fa-fw" aria-hidden="true"></i>
    <p>发布于：Apr 27, 2021</p>
  </a>
</div>

          
        
          
            
  <div class="new-meta-item browse leancloud">
    <a class='notlink'>
      
      <div id="lc-pv" data-title="恶意URL检测(九)一些比较杂的文章" data-path="/2021/04/27/%E6%81%B6%E6%84%8FURL%E6%A3%80%E6%B5%8B%E4%B9%9D/">
        <i class="fas fa-eye fa-fw" aria-hidden="true"></i>
        <span id='number'><i class="fas fa-circle-notch fa-spin fa-fw" aria-hidden="true"></i></span>
        次浏览
      </div>
    </a>
  </div>


          
        
      </div>
    
  </div>


  
  <h2 id="二十四、训练Transformers"><a href="#二十四、训练Transformers" class="headerlink" title="二十四、训练Transformers"></a>二十四、训练Transformers</h2><p>1、发表于火眼上的一篇文章，地址<a target="_blank" rel="noopener" href="https://www.fireeye.com/blog/threat-research/2021/01/training-transformers-for-cyber-security-tasks-malicious-url-prediction.html%EF%BC%8C%E6%97%B6%E9%97%B42021%E5%B9%B41%E6%9C%88%E3%80%82">https://www.fireeye.com/blog/threat-research/2021/01/training-transformers-for-cyber-security-tasks-malicious-url-prediction.html，时间2021年1月。</a></p>
<p>2、前人工作</p>
<p>GPT-3模型可以产生语法正确的长segments</p>
<p>…等等等</p>
<p>但是危害是：可能产生并传播很多无意义的信息</p>
<p>3、操作是在字符level，每一个字符与一个input token相关。</p>
<p>整体流程如下：</p>
<p><img src="/2021/04/27/%E6%81%B6%E6%84%8FURL%E6%A3%80%E6%B5%8B%E4%B9%9D/image-20210422085212777.png" class="lazyload" data-srcset="/2021/04/27/%E6%81%B6%E6%84%8FURL%E6%A3%80%E6%B5%8B%E4%B9%9D/image-20210422085212777.png" srcset="data:image/png;base64,666" alt="image-20210422085212777"></p>
<p>4、损失函数 &amp; 训练Regimes</p>
<p>Transformers允许自监督训练(可以看成是无监督的学习)，可以将transformer看作是特征提取器。</p>
<p>regimes:</p>
<p>(1) Direct Label Prediction (Decode-To-Label):   transformwes提取特征，然后输入到分类器计算。使用二值交叉熵损失。</p>
<p>(2) Next-Character Prediction Pre-Training and Fine-Tuning：类似预测下一个单词，可以利用无标签的数据。由于是多分类，必须用softmax函数。</p>
<p>(3) Balanced Mixed-Objective Training：使用到更多的知识。本文同时训练二值分类和下个特征分类。损失函数也是二者的集合。</p>
<p>5、实验结果</p>
<p>实验发现，Balanced Mixed-Objective Training效果最好，Direct Label Prediction次之。</p>
<p>由与其他模型如RF、CNN、LSTM等。</p>
<p>结果是CNN效果最好，得出结论Transformer可能不是那么适用于恶意URL检测。。。啊这 =。=</p>
<p>收获：</p>
<p>1、借鉴这种：同时使用二值分类和下个特征分类的思想。</p>
<p>但是本文得出的结论是：下个特征分类其实和恶意URL检测没有太大关系，所以可以考虑一些其他合适的方法。但是损失函数中引入他，却提高了性能。</p>
<h2 id="二十五、区块链-钓鱼检测"><a href="#二十五、区块链-钓鱼检测" class="headerlink" title="二十五、区块链 钓鱼检测"></a>二十五、区块链 钓鱼检测</h2><p>原文题目：Phishing Scam Detection on Ethereum: Towards Financial Security for Blockchain.</p>
<p>(其实本文不是太NLP)</p>
<p>1、文章发表于IJCAI2020，属于CCF- A类会议。作者来自中山大学。</p>
<p>2、在区块链领域，不仅有窃取隐私数据，还有诱骗用户往特定账户赚钱。非法加密的货币需要通过交易才能变成合法货币。公共区块链的交易记录是可以公开访问的，可能造成钓鱼攻击。</p>
<p>3、区块链中定位钓鱼账户存在的问题：</p>
<ol>
<li><p>只有交易记录，对账户的信息了解较少。</p>
</li>
<li><p>钓鱼地址较少，而其他地址非常多，定位钓鱼地址就显得比较困难。</p>
</li>
</ol>
<p>4、本文方法：</p>
<p><img src="/2021/04/27/%E6%81%B6%E6%84%8FURL%E6%A3%80%E6%B5%8B%E4%B9%9D/frame.jpg" class="lazyload" data-srcset="/2021/04/27/%E6%81%B6%E6%84%8FURL%E6%A3%80%E6%B5%8B%E4%B9%9D/frame.jpg" srcset="data:image/png;base64,666" alt="frame"></p>
<p>本文是基于对区块链的处理去检测钓鱼用户。提出了一种基于图的瀑布特征提取方法，以及一种基于lightGBM的双采样嵌入算法去构建模型。</p>
<p>5、瀑布特征提取方法</p>
<p>基于交易记录创建一个有向图TG。边有两个属性：blocknumber和amout，表示边出现的时间 和 交易的数量。</p>
<p><img src="/2021/04/27/%E6%81%B6%E6%84%8FURL%E6%A3%80%E6%B5%8B%E4%B9%9D/2fig2.jpg" class="lazyload" data-srcset="/2021/04/27/%E6%81%B6%E6%84%8FURL%E6%A3%80%E6%B5%8B%E4%B9%9D/2fig2.jpg" srcset="data:image/png;base64,666" alt="2fig2"></p>
<p>提取A的2阶朋友的特征，有如下3个步骤：</p>
<p>(1) 使用交易记录，计算2阶朋友的统计特征。</p>
<p>(2) 使用(1)的结果，计算1阶朋友的统计特征。而不是用1阶朋友本身的数据来计算的。</p>
<p>(3) 使用(2)的结果计算A节点的统计信息。</p>
<p>此处没有考虑边的方向～但是边的信息是很重要的，所以在两个方向上分别提取特征。</p>
<p><strong>结点特征：</strong>是这个节点的统计数据。有两种类型的数据：交易数量和交易时间。特征的命名格式：direction_type_method。例如：direction: in / out，type: block / ，method：std</p>
<p>最后一共得到了19个特征：</p>
<p>交易时间：</p>
<p>span: ptp</p>
<p>standard deviation: sd</p>
<p>交易数量：</p>
<p>sum</p>
<p>max</p>
<p>min</p>
<p>mean</p>
<p>standard deviation</p>
<p>交易不相关信息</p>
<p>count(交易数量)</p>
<p>unique(交易双方)</p>
<p>以上特征都是在in/out方向上分别计算，就是2*(2+5+2)=19</p>
<p>最后再算一个unique_ratio = unique/count。共19个特征～</p>
<p><strong>朋友节点特征</strong>：本文只计算一阶朋友节点。命名如下：friend_direction_statistic2_statistic1。</p>
<p>最后得到2*2 *2 * 5 *5= 200个特征。</p>
<p>6、双采样嵌入算法框架</p>
<p>(1) 基础模型</p>
<p>gradient boosting decision tree (GBDT)，包括XGBoost 和 lightGBM。本文发现在钓鱼检测方面，lightGBM更加有效，选择它作为基础模型。</p>
<p>具体模型的一些实现此处不总结了，也看不懂 TT</p>
<p>从训练集中移除掉较小的梯度samples。参考GOSS。</p>
<p>构建 CART regression tree的时候，lightGBM绑定互斥的特征，可以减少特征的数量。</p>
<p>(2)双采样嵌入</p>
<p>本人的启发思想来自[2008]EasyEnsemble，目的是解决分类不均衡的问题。伪代码如下：</p>
<p><img src="/2021/04/27/%E6%81%B6%E6%84%8FURL%E6%A3%80%E6%B5%8B%E4%B9%9D/2fig3precode.png" class="lazyload" data-srcset="/2021/04/27/%E6%81%B6%E6%84%8FURL%E6%A3%80%E6%B5%8B%E4%B9%9D/2fig3precode.png" srcset="data:image/png;base64,666" alt="2fig3precode"></p>
<p>主要的思想就是：主要在数据量大大数据集上取样。本文的差异是，也在训练集上取样。</p>
<p>7、数据</p>
<p>钓鱼网站(1683个)：<a target="_blank" rel="noopener" href="https://etherscan.io/accounts/label/phish-hack">https://etherscan.io/accounts/label/phish-hack</a> </p>
<p>合法数据：<a target="_blank" rel="noopener" href="https://www.parity.io/ethereum/">https://www.parity.io/ethereum/</a></p>
<p>由于两者数据量差别较大，需要进行数据清洗。将一些很明显的良性数据删除掉，不进行训练。</p>
<p>本文1)过滤掉了智能合约的地址，2) 过滤掉少于10个或多于1000条交易记录的账户 3) 忽视掉在block height 2百万之前的记录。</p>
<p>原因是：1) 智能合约通常逻辑复杂，不易执行钓鱼检测，并且智能合约中只有很小一部分(2.6%)在钓鱼检测中，通常与tokens相关。2)  交易记录过少，不易评价，过多，则账户可能是wallet 或其他账户类型。事实上，有超过70%的账户都有超过1000条交易记录，但是只有1个是被标记为钓鱼的。 3) 分析发现钓鱼地址的活动时间都在2016-08-02之后，可能是早期钓鱼账户较少，记录就更少了。</p>
<p>最终选取了534，820个地址，其中323个是钓鱼地址。</p>
<p>8、实验结果</p>
<p>采用了k-fold交叉验证，其中k = 5</p>
<p>采用4个指标评价：precision / recall / F1 / AUC。</p>
<p>模型比较：比较lightGBM、SVM、DT和双采样嵌入模型(DE+)。设定特征采样率为70%，基础模型的数量为1600，结果如下。其中SVM效果最差。加入DE+之后，这三个模型效果都提升了。</p>
<p><img src="/2021/04/27/%E6%81%B6%E6%84%8FURL%E6%A3%80%E6%B5%8B%E4%B9%9D/2fig4resultmodel.png" class="lazyload" data-srcset="/2021/04/27/%E6%81%B6%E6%84%8FURL%E6%A3%80%E6%B5%8B%E4%B9%9D/2fig4resultmodel.png" srcset="data:image/png;base64,666" alt="2fig4resultmodel"></p>
<p>采样的有效性分析：</p>
<p>​    基模型的数量对结果的影响。</p>
<p><img src="/2021/04/27/%E6%81%B6%E6%84%8FURL%E6%A3%80%E6%B5%8B%E4%B9%9D/2fig5.png" class="lazyload" data-srcset="/2021/04/27/%E6%81%B6%E6%84%8FURL%E6%A3%80%E6%B5%8B%E4%B9%9D/2fig5.png" srcset="data:image/png;base64,666" alt="2fig5"></p>
<p>特征采样评估</p>
<p>​    设定不同的采样率，且基模型的数量设为1600。</p>
<p><img src="/2021/04/27/%E6%81%B6%E6%84%8FURL%E6%A3%80%E6%B5%8B%E4%B9%9D/2fig6.png" class="lazyload" data-srcset="/2021/04/27/%E6%81%B6%E6%84%8FURL%E6%A3%80%E6%B5%8B%E4%B9%9D/2fig6.png" srcset="data:image/png;base64,666" alt="2fig6"></p>
<p>0.8效果最好。说明并不是特征却多越好。如果特征太多，特征采样的方法可能会提高性能。这可能是因为 feature sampling can make different base models view the object from different angles,</p>
<p>这个因素有一定影响，但影响没有上一个因素那么大。</p>
<p>9、特征分析</p>
<p>top 15特征如下：</p>
<p><img src="/2021/04/27/%E6%81%B6%E6%84%8FURL%E6%A3%80%E6%B5%8B%E4%B9%9D/2fig7.png" class="lazyload" data-srcset="/2021/04/27/%E6%81%B6%E6%84%8FURL%E6%A3%80%E6%B5%8B%E4%B9%9D/2fig7.png" srcset="data:image/png;base64,666" alt="2fig7"></p>
<p>in_block_std：反应了某个特定地址的in交易的强度。钓鱼网站的in交易突然增加。然而当钓鱼网站被披露之后，in交易会变少甚至消失。这导致钓鱼网站的in交易基本上都集中在某一个时间段内。</p>
<p>to_out_sum_median：median可以看成指代着经济strength，对于钓鱼地址，to 朋友是钓鱼网址的受害者，这个特征指代的是受害者基经济水平。</p>
<p>from_in_sum_min：钓鱼网站通常需要洗钱，这个特征通常作为过渡的地址，行为与正常地址很不一样。</p>
<p>收获：</p>
<p>1、代码开源！— 没找到代码，借鉴这种思想吧！</p>
<p>2、针对恶意样本较少的问题，能否采用本文的双采样方法？如果为了平衡数据，可以选择良性里面看起来不那么良的，以增加模型的识别能力。</p>
<p>3、如果特征太多，可以采用采样的方法。考虑能否用到n-grams里面。</p>
<h2 id="二十六、XLNet：语言理解的广义回归预训练"><a href="#二十六、XLNet：语言理解的广义回归预训练" class="headerlink" title="二十六、XLNet：语言理解的广义回归预训练"></a>二十六、XLNet：语言理解的广义回归预训练</h2><p>1、arXiv上的文章，作者来自卡内基梅隆大学，谷歌人工智能大脑团队。</p>
<p>2、预训练</p>
<p>AR：自回归，用一个回归模型来估计文本语料库的概率分布。其只被训练为编码单向上下文（向前或向后）</p>
<p>AE：自动编码，从损坏的输入重建原始数据。例如BERT。可以双向。</p>
<p>本文用到了AR和AE的优点。</p>
<p>3、本文方法：</p>
<p>pretraining objective：</p>
<p>(1)AR中，没有使用固定的因式分解阶，而是最大化一个序列的期望对数似然，也就是 所有可能的因子分解顺序排列。</p>
<p>(2)且本文的XLNet不依赖于数据损坏。同时，自回归目标还提供了一种自然的方法 来使用乘积规则分解预测令牌的联合概率，消除了BERT中的独立性假设。</p>
<p>architectural designs for pretraining：</p>
<p>(1) XLNet将Transformer XL的段递归机制和相关编码方案[9]集成到预训练中。</p>
<p>(2) 单纯地将Transformer（-XL）体系结构应用于基于置换的语言建模是行不通的，因为分解顺序是任意的，目标是不明确的。解决方案是：重新为Transformer设定参数。</p>
<p>收获：</p>
<p>1、能否理解URL的含义？</p>
<p>2、之前没了解过nlp，这篇文章基本都看不懂，代码开源，但是也没去看。感觉方向还不太重合，没有找到太多可以借鉴的思想。</p>
<h2 id="二十七、Universal-Sentence-Encoder-通用句子编码器"><a href="#二十七、Universal-Sentence-Encoder-通用句子编码器" class="headerlink" title="二十七、Universal Sentence Encoder 通用句子编码器"></a>二十七、Universal Sentence Encoder 通用句子编码器</h2><p>1、arXiv上的文章，作者来自谷歌。</p>
<p>2、可用的数据集不够，许多模型通过使用预先训练好的单词嵌入（如word2vec（Mikolov et al.，2013）或GloVe（Pennington et al.，2014）生成的单词嵌入，通过隐式形成有限的迁移学习来解决这个问题。</p>
<p>3、文章中还有较多内容，但是先不看了，重点看一下模型怎么使用</p>
<p>代码实现</p>
<p>参考<a target="_blank" rel="noopener" href="https://colab.research.google.com/github/tensorflow/hub/blob/50bbebaa248cff13e82ddf0268ed1b149ef478f2/examples/colab/semantic_similarity_with_tf_hub_universal_encoder.ipynb#scrollTo=MSeY-MUQo2Ha">https://colab.research.google.com/github/tensorflow/hub/blob/50bbebaa248cff13e82ddf0268ed1b149ef478f2/examples/colab/semantic_similarity_with_tf_hub_universal_encoder.ipynb#scrollTo=MSeY-MUQo2Ha</a></p>
<p>ipython中</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#导入库</span></span><br><span class="line"><span class="keyword">from</span> absl <span class="keyword">import</span> logging</span><br><span class="line"></span><br><span class="line"><span class="comment">#import tensorflow as tf</span></span><br><span class="line"><span class="keyword">import</span> tensorflow.compat.v1 <span class="keyword">as</span> tf</span><br><span class="line">tf.disable_v2_behavior( )</span><br><span class="line"><span class="keyword">import</span> tensorflow_hub <span class="keyword">as</span> hub</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">import</span> re</span><br><span class="line"><span class="keyword">import</span> seaborn <span class="keyword">as</span> sns</span><br><span class="line"></span><br><span class="line"><span class="comment">#载入模型</span></span><br><span class="line">module_url = <span class="string">&quot;https://tfhub.dev/google/universal-sentence-encoder/4&quot;</span></span><br><span class="line">model = hub.load(module_url)</span><br><span class="line"><span class="keyword">print</span> (<span class="string">&quot;module %s loaded&quot;</span> % module_url)</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">embed</span>(<span class="params">input</span>):</span></span><br><span class="line">  <span class="keyword">return</span> model(input)</span><br></pre></td></tr></table></figure>

<p>url载入模型失败，总显示时间超时。</p>
<p>想要下载.pb模型以及变量文件然后导入，但是还是有点问题再加上服务器传不了很大的文件。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> tensorflow.python.platform <span class="keyword">import</span> gfile</span><br><span class="line"> </span><br><span class="line"><span class="comment">#sess = tf.Session()</span></span><br><span class="line">sess = tf.compat.v1.InteractiveSession()</span><br><span class="line"><span class="keyword">with</span> gfile.FastGFile(<span class="string">&#x27;saved_model.pb&#x27;</span>, <span class="string">&#x27;rb&#x27;</span>) <span class="keyword">as</span> f:</span><br><span class="line">    graph_def = tf.compat.v1.GraphDef.FromString(file_handle.read())</span><br><span class="line">    graph_def.ParseFromString(f.read())</span><br><span class="line">    sess.graph.as_default()</span><br><span class="line">    tf.import_graph_def(graph_def, name=<span class="string">&#x27;&#x27;</span>)</span><br><span class="line"></span><br></pre></td></tr></table></figure>



<p>在colab中运行吧～</p>
<p>以下主要体现的是不管长句子还是短句子都可以～</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#@title Compute a representation for each message, showing various lengths supported.</span></span><br><span class="line">word = <span class="string">&quot;Elephant&quot;</span></span><br><span class="line">sentence = <span class="string">&quot;I am a sentence for which I would like to get its embedding.&quot;</span></span><br><span class="line">paragraph = (</span><br><span class="line">    <span class="string">&quot;Universal Sentence Encoder embeddings also support short paragraphs. &quot;</span></span><br><span class="line">    <span class="string">&quot;There is no hard limit on how long the paragraph is. Roughly, the longer &quot;</span></span><br><span class="line">    <span class="string">&quot;the more &#x27;diluted&#x27; the embedding will be.&quot;</span>)</span><br><span class="line">messages = [word, sentence, paragraph]</span><br><span class="line"></span><br><span class="line"><span class="comment"># Reduce logging output.</span></span><br><span class="line">logging.set_verbosity(logging.ERROR)</span><br><span class="line"></span><br><span class="line">message_embeddings = embed(messages)</span><br><span class="line"></span><br><span class="line"><span class="comment">#message_embeddings结果如下</span></span><br><span class="line">tf.Tensor(</span><br><span class="line">[[ <span class="number">0.00834449</span>  <span class="number">0.00048082</span>  <span class="number">0.06595246</span> ... <span class="number">-0.03266348</span>  <span class="number">0.02640911</span></span><br><span class="line">  <span class="number">-0.0606688</span> ]</span><br><span class="line"> [ <span class="number">0.05080861</span> <span class="number">-0.01652431</span>  <span class="number">0.01573779</span> ...  <span class="number">0.00976659</span>  <span class="number">0.03170122</span></span><br><span class="line">   <span class="number">0.01788117</span>]</span><br><span class="line"> [<span class="number">-0.0283327</span>  <span class="number">-0.05586218</span> <span class="number">-0.01294146</span> ... <span class="number">-0.05133027</span>  <span class="number">0.01178872</span></span><br><span class="line">   <span class="number">0.00579202</span>]], shape=(<span class="number">3</span>, <span class="number">512</span>), dtype=float32)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> i, message_embedding <span class="keyword">in</span> enumerate(np.array(message_embeddings).tolist()):</span><br><span class="line">  print(<span class="string">&quot;Message: &#123;&#125;&quot;</span>.format(messages[i]))</span><br><span class="line">  print(<span class="string">&quot;Embedding size: &#123;&#125;&quot;</span>.format(len(message_embedding)))</span><br><span class="line">  message_embedding_snippet = <span class="string">&quot;, &quot;</span>.join(</span><br><span class="line">      (str(x) <span class="keyword">for</span> x <span class="keyword">in</span> message_embedding[:<span class="number">3</span>]))</span><br><span class="line">  print(<span class="string">&quot;Embedding: [&#123;&#125;, ...]\n&quot;</span>.format(message_embedding_snippet))</span><br></pre></td></tr></table></figure>

<p>结果如下</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">Message: Elephant</span><br><span class="line">Embedding size: <span class="number">512</span></span><br><span class="line">Embedding: [<span class="number">0.008344488218426704</span>, <span class="number">0.00048081763088703156</span>, <span class="number">0.06595246493816376</span>, ...]</span><br><span class="line"></span><br><span class="line">Message: I am a sentence <span class="keyword">for</span> which I would like to get its embedding.</span><br><span class="line">Embedding size: <span class="number">512</span></span><br><span class="line">Embedding: [<span class="number">0.0508086122572422</span>, <span class="number">-0.016524313017725945</span>, <span class="number">0.015737786889076233</span>, ...]</span><br><span class="line"></span><br><span class="line">Message: Universal Sentence Encoder embeddings also support short paragraphs. There <span class="keyword">is</span> no hard limit on how long the paragraph <span class="keyword">is</span>. Roughly, the longer the more <span class="string">&#x27;diluted&#x27;</span> the embedding will be.</span><br><span class="line">Embedding size: <span class="number">512</span></span><br><span class="line">Embedding: [<span class="number">-0.02833268605172634</span>, <span class="number">-0.055862169712781906</span>, <span class="number">-0.012941470369696617</span>, ...]</span><br></pre></td></tr></table></figure>



<p>以下可以计算两个句子的相似度</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">plot_similarity</span>(<span class="params">labels, features, rotation</span>):</span></span><br><span class="line">  corr = np.inner(features, features)	<span class="comment">#计算内积，以行为单位来进行计算的</span></span><br><span class="line">  sns.set(font_scale=<span class="number">1.2</span>)</span><br><span class="line">  g = sns.heatmap(</span><br><span class="line">      corr,</span><br><span class="line">      xticklabels=labels,</span><br><span class="line">      yticklabels=labels,</span><br><span class="line">      vmin=<span class="number">0</span>,	<span class="comment">#设定最小值和最大值</span></span><br><span class="line">      vmax=<span class="number">1</span>,	</span><br><span class="line">      cmap=<span class="string">&quot;YlOrRd&quot;</span>)</span><br><span class="line">  g.set_xticklabels(labels, rotation=rotation)</span><br><span class="line">  g.set_title(<span class="string">&quot;Semantic Textual Similarity&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">run_and_plot</span>(<span class="params">messages_</span>):</span></span><br><span class="line">  message_embeddings_ = embed(messages_)	<span class="comment">#进行嵌入</span></span><br><span class="line">  plot_similarity(messages_, message_embeddings_, <span class="number">90</span>)	<span class="comment">#画图</span></span><br></pre></td></tr></table></figure>

<p>其实本质上就是把原来的式子计算内积，相应的位置内积的值越大，说明相关性越强</p>
<p>semantic textual 相似性(STS) 基准评估</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="comment">#sts_dev如下：</span></span><br><span class="line">sts_data :         sim  ...                                             sent_2</span><br><span class="line"><span class="number">0</span>     <span class="number">5.00</span>  ...               A man wearing a hard hat <span class="keyword">is</span> dancing.</span><br><span class="line"><span class="number">1</span>     <span class="number">4.75</span>  ...                         A child <span class="keyword">is</span> riding a horse.</span><br><span class="line"><span class="number">2</span>     <span class="number">5.00</span>  ...           The man <span class="keyword">is</span> feeding a mouse to the snake.</span><br><span class="line"><span class="number">3</span>     <span class="number">2.40</span>  ...                           A man <span class="keyword">is</span> playing guitar.</span><br><span class="line"><span class="number">4</span>     <span class="number">2.75</span>  ...                          A man <span class="keyword">is</span> playing a flute.</span><br><span class="line"><span class="meta">... </span>   ...  ...                                                ...</span><br><span class="line">1465  2.00  ...                 Has Nasa discovered water on Mars?</span><br><span class="line"><span class="number">1466</span>  <span class="number">0.00</span>  ...     WTO: India regrets action of developed nations</span><br><span class="line"><span class="number">1467</span>  <span class="number">2.00</span>  ...  Volkswagen<span class="string">&#x27;s &quot;gesture of goodwill&quot; to diesel o...</span></span><br><span class="line"><span class="string">1468  0.00  ...  Obama waiting for midterm to name attorney gen...</span></span><br><span class="line"><span class="string">1469  0.00  ...  New York police officer critically wounded in ...</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">[1468 rows x 3 columns]</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">#dev_scores如下：</span></span><br><span class="line"><span class="string">dev_scores: [5.0, 4.75, 5.0, 2.4, 2.75, 2.615, 5.0, 2.333, 3.75, 5.0, 3.2, 1.5830000000000002, 5.0, 5.0, 4.909, 0.8, 2.4, 5.0, 4.0, 0.636, 3.0, 1.714, 3.2, 2.167, 1.0, 1.9169999999999998, 4.25, 3.0, 1.0, 0.6, 2.6, 5.0, 4.6, 5.0, 4.8, 3.8, 5.0, 5.0, 4.2, 1.4, 3.6, 2.8, 1.6, 3.0, 1.4, 0.25, 0.25, 0.0, 4.0, 4.5, 0.5, 3.8, 4.8, 5.0, 0.25, 1.2, 0.6, 0.8, 3.8, 0.0, 3.5, 4.5, 2.8, 3.8, 3.8, 0.0, 4.0, 4.25, 2.812, 4.25, 3.0, 1.0, 3.75, 0.0, 0.4, 4.0, 2.6, 0.8, 2.4, 1.0, 0.2, 0.6, 3.6, 0.2, 0.2, 0.0, 0.0, 1.2, 0.0, 0.4, 0.4, 3.0, 4.4, 3.2, 3.4, 3.4, 3.6, 0.8, 0.8, 1.0, 1.6, 1.0, 3.2, 2.0, 0.0, 0.6, 0.0, 1.4, 2.0, 2.6, 4.2, 2.8, 0.0, 0.0.6, 0.0, 0.0, 0.0, 0.0, 2.2, 0.0, 0.4, 0.0, 2.6, 0.8, 0.4, 3.0, 2.2, 0.6, 3.4, 2.2, 1.8, 0.8, </span></span><br><span class="line"><span class="string">             ......</span></span><br><span class="line"><span class="string">4.6, 3.8, 1.4, 3.8, 0.0, 1.6, 5.0, 0.4, 2.4, 3.2, 3.8, 2.6, 1.0, 4.0, 2.0, 2.2, 4.6, 4.4, 1.8, 0.0, 0.0, 2.0, 3.4, 0.2, 2.4, 0.0, 0.4, 1.8, 0.8, 0.4, 2.2, 1.8, 4.6, 2.2, 1.0, 1.0, 4.2, 0.4, 0.2, 0.0, 0.0, 0.0, 0.6, 0.6, 4.0, 0.0, 0.8, 3.2, 4.8, 4.2, 2.2, 1.0, 3.6, 1.6, 3.0, 2.4, 0.4, 0.0, 5.0, 1.2, 1.4, 4.0, 4.6, 3.4, 0.4, 0.0, 4.8, 5.0, 4.0, 4.0, 3.0, 5.0, 2.0, 3.0, 3.0, 1.0, 5.0, 2.0, 2.0, 2.0, 5.0, 3.0, 3.0, 0.0, 2.0, 0.0, 2.0, 0.0, 0.0]</span></span><br><span class="line"><span class="string">Pearson correlation coefficient = 0.8036389313002914</span></span><br><span class="line"><span class="string">p-value = 0.0</span></span><br></pre></td></tr></table></figure>



<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">sts_data = sts_dev <span class="comment">#@param [&quot;sts_dev&quot;, &quot;sts_test&quot;] &#123;type:&quot;raw&quot;&#125;</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">run_sts_benchmark</span>(<span class="params">batch</span>):</span></span><br><span class="line">  sts_encode1 = tf.nn.l2_normalize(embed(tf.constant(batch[<span class="string">&#x27;sent_1&#x27;</span>].tolist())), axis=<span class="number">1</span>)</span><br><span class="line">  sts_encode2 = tf.nn.l2_normalize(embed(tf.constant(batch[<span class="string">&#x27;sent_2&#x27;</span>].tolist())), axis=<span class="number">1</span>)</span><br><span class="line">  cosine_similarities = tf.reduce_sum(tf.multiply(sts_encode1, sts_encode2), axis=<span class="number">1</span>)</span><br><span class="line">  clip_cosine_similarities = tf.clip_by_value(cosine_similarities, <span class="number">-1.0</span>, <span class="number">1.0</span>)</span><br><span class="line">  scores = <span class="number">1.0</span> - tf.acos(clip_cosine_similarities) / math.pi</span><br><span class="line">  <span class="string">&quot;&quot;&quot;Returns the similarity scores&quot;&quot;&quot;</span></span><br><span class="line">  <span class="keyword">return</span> scores</span><br><span class="line"></span><br><span class="line">dev_scores = sts_data[<span class="string">&#x27;sim&#x27;</span>].tolist()</span><br><span class="line">scores = []</span><br><span class="line"><span class="keyword">for</span> batch <span class="keyword">in</span> np.array_split(sts_data, <span class="number">10</span>):	<span class="comment">#分成10份</span></span><br><span class="line">  scores.extend(run_sts_benchmark(batch))</span><br><span class="line"></span><br><span class="line">pearson_correlation = scipy.stats.pearsonr(scores, dev_scores)</span><br><span class="line">print(<span class="string">&#x27;Pearson correlation coefficient = &#123;0&#125;\np-value = &#123;1&#125;&#x27;</span>.format(</span><br><span class="line">    pearson_correlation[<span class="number">0</span>], pearson_correlation[<span class="number">1</span>]))</span><br></pre></td></tr></table></figure>



<p>自己尝试用3个URL去编码，结果如下</p>
<p><img src="/2021/04/27/%E6%81%B6%E6%84%8FURL%E6%A3%80%E6%B5%8B%E4%B9%9D/3fig1.png" class="lazyload" data-srcset="/2021/04/27/%E6%81%B6%E6%84%8FURL%E6%A3%80%E6%B5%8B%E4%B9%9D/3fig1.png" srcset="data:image/png;base64,666" alt="3fig1"></p>
<p>如果对URL进行以下预处理，.、/、等号、变空格，-_直接删除</p>
<p>良性的结果好很多，恶意有点离谱～</p>
<p><img src="/2021/04/27/%E6%81%B6%E6%84%8FURL%E6%A3%80%E6%B5%8B%E4%B9%9D/3fig2.png" class="lazyload" data-srcset="/2021/04/27/%E6%81%B6%E6%84%8FURL%E6%A3%80%E6%B5%8B%E4%B9%9D/3fig2.png" srcset="data:image/png;base64,666" alt="3fig2"></p>
<h2 id="二十八、通过网页中的-6-个特征字段检测钓鱼网站"><a href="#二十八、通过网页中的-6-个特征字段检测钓鱼网站" class="headerlink" title="二十八、通过网页中的 6 个特征字段检测钓鱼网站"></a>二十八、通过网页中的 6 个特征字段检测钓鱼网站</h2><p>发表于嘶吼的公众号，链接<a target="_blank" rel="noopener" href="https://mp.weixin.qq.com/s/qQUQLlrALkBz-adWz56BTg">https://mp.weixin.qq.com/s/qQUQLlrALkBz-adWz56BTg</a></p>
<p>1、检测网页的HTML片段。</p>
<p>攻击者复制网站的时候，可能复制了一些无用的信息，我们的目标就是检测出这些无意义、被复制进去了的信息。</p>
<p>这些无意义的文件可能是哈希、版本控制、SaaS的API密钥、CSRF令牌、内容安全策略(CSP)的随机数、子资源完整性哈希。</p>
<p>这种想法挺巧妙的！</p>
<h2 id="二十九、使用词汇特征检测钓鱼网站"><a href="#二十九、使用词汇特征检测钓鱼网站" class="headerlink" title="二十九、使用词汇特征检测钓鱼网站"></a>二十九、使用词汇特征检测钓鱼网站</h2><p>1、文章发表于Computer Communications，2021。</p>
<p>2、选取9个最重要的特征，使用算法Spearman correlation, K best, and Random forest计算特征权重，然后输入到机器学习算法中。</p>
<p><img src="/2021/04/27/%E6%81%B6%E6%84%8FURL%E6%A3%80%E6%B5%8B%E4%B9%9D/4fig1.png" class="lazyload" data-srcset="/2021/04/27/%E6%81%B6%E6%84%8FURL%E6%A3%80%E6%B5%8B%E4%B9%9D/4fig1.png" srcset="data:image/png;base64,666" alt="4fig1"></p>
<p>特征如下：</p>
<p>(1) 域中的token数</p>
<p>百度发现这个可以直接用parse来实现</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> urllib.parse <span class="keyword">import</span> urlparse</span><br><span class="line">url = <span class="string">&#x27;http://netloc/path;param?query=arg#frag&#x27;</span></span><br><span class="line">parsed = urlparse(url)</span><br><span class="line">print(parsed)</span><br><span class="line">print(len(parsed))</span><br></pre></td></tr></table></figure>

<p>结果如下</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">url = <span class="string">&#x27;http://netloc/path;param?query=arg#frag&#x27;</span></span><br><span class="line"></span><br><span class="line">ParseResult(scheme=<span class="string">&#x27;http&#x27;</span>, netloc=<span class="string">&#x27;netloc&#x27;</span>, path=<span class="string">&#x27;/path&#x27;</span>, params=<span class="string">&#x27;param&#x27;</span>, query=<span class="string">&#x27;query=arg&#x27;</span>, fragment=<span class="string">&#x27;frag&#x27;</span>)</span><br><span class="line"></span><br><span class="line">In [<span class="number">20</span>]: len(parsed)</span><br><span class="line">Out[<span class="number">20</span>]: <span class="number">6</span></span><br></pre></td></tr></table></figure>



<p>(2) 顶级域名的数量 TLD</p>
<p>自己保存一个顶级域名列表，如果在这个列表中的才算做是顶级域名。</p>
<p>别的第三方库好像不能用，因为他们只能提取出一个顶级域，不能提取出多个。只能自己写个正则表达式的字符串匹配。</p>
<p>实现思路：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">.出现</span><br><span class="line">匹配后面的字母</span><br><span class="line">在库中搜索</span><br></pre></td></tr></table></figure>

<p>…</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> urllib.parse <span class="keyword">import</span> urlparse</span><br><span class="line"><span class="keyword">import</span> re</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">print(<span class="string">&quot;--&quot;</span>*<span class="number">40</span>)</span><br><span class="line"><span class="keyword">for</span> url <span class="keyword">in</span> urls:</span><br><span class="line">  <span class="keyword">print</span></span><br><span class="line">    parts = urlparse(url)</span><br><span class="line">    host = parts.netloc</span><br><span class="line">    m = pattern.search(host)</span><br><span class="line">    res =  m.group() <span class="keyword">if</span> m <span class="keyword">else</span> host</span><br><span class="line">    print(<span class="string">&quot;unkonw&quot;</span>) <span class="keyword">if</span> <span class="keyword">not</span> res <span class="keyword">else</span> res</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p>库</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[a-z0<span class="number">-9</span>-]&#123;<span class="number">1</span>,<span class="number">63</span>&#125;(.ab.ca|.bc.ca|.mb.ca|.nb.ca|.nf.ca|.nl.ca|.ns.ca|.nt.ca|.nu.ca|.on.ca|.pe.ca|.qc.ca|.sk.ca|.yk.ca|.co.cc|.com.cd|.net.cd|.org.cd|.co.ck|.ac.cn|.com.cn|.edu.cn|.gov.cn|.net.cn|.org.cn|.ah.cn|.bj.cn|.cq.cn|.fj.cn|.gd.cn|.gs.cn|.gz.cn|.gx.cn|.ha.cn|.hb.cn|.he.cn|.hi.cn|.hl.cn|.hn.cn|.jl.cn|.js.cn|.jx.cn|.ln.cn|.nm.cn|.nx.cn|.qh.cn|.sc.cn|.sd.cn|.sh.cn|.sn.cn|.sx.cn|.tj.cn|.xj.cn|.xz.cn|.yn.cn|.zj.cn|.us.com|.com.cu|.edu.cu|.org.cu|.net.cu|.gov.cu|.inf.cu|.gov.cx|.com.dz|.org.dz|.net.dz|.gov.dz|.edu.dz|.asso.dz|.pol.dz|.art.dz|.com.ec|.info.ec|.net.ec|</span><br><span class="line"><span class="meta">... </span>               .win|.work|.wtf|.xxx|.XYZ|.kaufen|.desi|.shiksha|.moda|.futbol|.juegos|.uno|.africa|.asia|.krd|.taipei|.tokyo|.alsace|.amsterdam|.bcn|.barcelona|.berlin|.brussels|.bzh|.cat|.cymru|.eus|.frl|.gal|.gent|.irish|.istanbul|.istanbul|.london|.paris|.saarland|.scot|.swiss|.wales|.wien|.miami|.nyc|.quebec|.vegas|.kiwi|.melbourne|.sydney|.lat|.rio|.ru|.aaa|.abb|.aeg|.afl|.aig|.airtel|.bbc|.bentley|.example|.invalid|.local|.localhost|.onion|.testa)$</span><br></pre></td></tr></table></figure>



<p>这个没写出来，在山穷水尽的时候发现github上有自动提取特征的代码！我又活了～</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> re</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">count_tld</span>(<span class="params">text</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;Return amount of Top-Level Domains (TLD) present in the URL.&quot;&quot;&quot;</span></span><br><span class="line">    file = open(<span class="string">&#x27;tlds.txt&#x27;</span>, <span class="string">&#x27;r&#x27;</span>)</span><br><span class="line">    count = <span class="number">0</span></span><br><span class="line">    pattern = re.compile(<span class="string">&quot;[a-zA-Z0-9.]&quot;</span>)</span><br><span class="line">    <span class="keyword">for</span> line <span class="keyword">in</span> file:</span><br><span class="line">        i = (text.lower().strip()).find(line.strip())</span><br><span class="line">        <span class="keyword">while</span> i &gt; <span class="number">-1</span>:</span><br><span class="line">            <span class="keyword">if</span> ((i + len(line) - <span class="number">1</span>) &gt;= len(text)) <span class="keyword">or</span> <span class="keyword">not</span> pattern.match(text[i + len(line) - <span class="number">1</span>]):</span><br><span class="line">                count += <span class="number">1</span></span><br><span class="line">            i = text.find(line.strip(), i + <span class="number">1</span>)</span><br><span class="line">    file.close()</span><br><span class="line">    <span class="keyword">return</span> count</span><br><span class="line"></span><br><span class="line">url = <span class="string">&quot;https://blog.csdn.net/weixin_42793426/article/details.xyz.cn/88545939.xyz&quot;</span></span><br><span class="line">count_tld(url)</span><br></pre></td></tr></table></figure>

<p>大致思路就是从库中一个一个去比对，看看有没有出现过～太巧妙了！</p>
<p>(3) URL的长度</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span>  <span class="title">length_url</span>(<span class="params">text</span>):</span></span><br><span class="line">		<span class="keyword">return</span> len(text)</span><br></pre></td></tr></table></figure>



<p>(4) 查询中的数据</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> urllib.parse <span class="keyword">import</span> urlparse</span><br><span class="line"><span class="keyword">import</span> re</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">count_digit_in_query</span>(<span class="params">text</span>):</span></span><br><span class="line">    content_query = urlparse(text).query</span><br><span class="line">    print(content_query)</span><br><span class="line">    count = <span class="number">0</span></span><br><span class="line">    <span class="keyword">return</span> len(re.findall(<span class="string">r&#x27;=[1-9][^a-z]&#x27;</span>, content_query))</span><br><span class="line"></span><br><span class="line">url = <span class="string">&quot;https://blog.csdn.net/weixin_42793426/article?id=4/details.xyz.cn/88545939.xyz?id=3cdjdvjdfbv/csjhvufhgui?id=11&quot;</span></span><br><span class="line">res = count_digit_in_query(url)</span><br><span class="line"><span class="keyword">if</span>(res == <span class="number">0</span>): res = <span class="number">-1</span></span><br></pre></td></tr></table></figure>

<p>这个也是自己写的，相当于在query中匹配 <code>=数字</code>，有一个缺陷就是?id=34bjsnfejrjv也会被匹配到，认为是数字查询。不过这种情况应该很少，就先不管他啦～</p>
<p>(5) URL中点的数量</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">count</span>(<span class="params">text, character</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;Return the amount of certain character in the text.&quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">return</span> text.count(character)</span><br><span class="line"></span><br><span class="line">url = <span class="string">&quot;http://127.0.0.1:800....0&quot;</span></span><br><span class="line">res = count(url, <span class="string">&quot;.&quot;</span>)</span><br></pre></td></tr></table></figure>



<p>(6) 域中定界符的数量</p>
<p>因为在域中可能会做一些混淆，使得一个URL看起来像是合法的URL。</p>
<p>先找到所有的域，再分别找到每个域中的定界符。但是找到所有的域有点难度，直接用urlparse的netloc。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> urllib.parse <span class="keyword">import</span> urlparse</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">count</span>(<span class="params">text, character</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;Return the amount of certain character in the text.&quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">return</span> text.count(character)</span><br><span class="line">url = <span class="string">&quot;https://bl-og.csdn.net/weixin_42793426/article?id=4/details.xyz.cn/88545939.xyz?id=3cdjdvjdfbv/csjhvufhgui?id=11&quot;</span></span><br><span class="line">domain = urlparse(url).netloc</span><br><span class="line">count_res = <span class="number">0</span>;</span><br><span class="line">delimiter_list = [<span class="string">&#x27;.&#x27;</span>,<span class="string">&#x27;-&#x27;</span>,<span class="string">&#x27;_&#x27;</span>,<span class="string">&#x27;/&#x27;</span>,<span class="string">&#x27;?&#x27;</span>,<span class="string">&#x27;=&#x27;</span>,<span class="string">&#x27;@&#x27;</span>,<span class="string">&#x27;&amp;&#x27;</span>,<span class="string">&#x27;!&#x27;</span>,<span class="string">&#x27; &#x27;</span>,<span class="string">&#x27;~&#x27;</span>,<span class="string">&#x27;,&#x27;</span>,<span class="string">&#x27;+&#x27;</span>,<span class="string">&#x27;*&#x27;</span>,<span class="string">&#x27;#&#x27;</span>,<span class="string">&#x27;$&#x27;</span>,<span class="string">&#x27;%&#x27;</span>,<span class="string">&#x27;;&#x27;</span>,<span class="string">&#x27;&#123;&#x27;</span>,<span class="string">&#x27;|&#x27;</span>,<span class="string">&#x27;(&#x27;</span>]</span><br><span class="line"><span class="keyword">for</span> symbol <span class="keyword">in</span> delimiter_list:</span><br><span class="line">  count_res += count(domain,symbol)</span><br></pre></td></tr></table></figure>

<p>如果后续改进的话，借助count_tld。if count_tld &gt; 1，后面再搜索一次</p>
<p>(7) path中定界符的数量</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> urllib.parse <span class="keyword">import</span> urlparse</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">count</span>(<span class="params">text, character</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;Return the amount of certain character in the text.&quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">return</span> text.count(character)</span><br><span class="line">url = <span class="string">&quot;https://bl-og.csdn.net/weixin_42793426/article?id=4/details.xyz.cn/88545939.xyz?id=3cdjdvjdfbv/csjhvufhgui?id=11&quot;</span></span><br><span class="line">path = urlparse(url).path</span><br><span class="line">count_res = <span class="number">0</span>;</span><br><span class="line">delimiter_list = [<span class="string">&#x27;.&#x27;</span>,<span class="string">&#x27;-&#x27;</span>,<span class="string">&#x27;_&#x27;</span>,<span class="string">&#x27;/&#x27;</span>,<span class="string">&#x27;?&#x27;</span>,<span class="string">&#x27;=&#x27;</span>,<span class="string">&#x27;@&#x27;</span>,<span class="string">&#x27;&amp;&#x27;</span>,<span class="string">&#x27;!&#x27;</span>,<span class="string">&#x27; &#x27;</span>,<span class="string">&#x27;~&#x27;</span>,<span class="string">&#x27;,&#x27;</span>,<span class="string">&#x27;+&#x27;</span>,<span class="string">&#x27;*&#x27;</span>,<span class="string">&#x27;#&#x27;</span>,<span class="string">&#x27;$&#x27;</span>,<span class="string">&#x27;%&#x27;</span>,<span class="string">&#x27;;&#x27;</span>,<span class="string">&#x27;&#123;&#x27;</span>,<span class="string">&#x27;|&#x27;</span>,<span class="string">&#x27;(&#x27;</span>]</span><br><span class="line"><span class="keyword">for</span> symbol <span class="keyword">in</span> delimiter_list:</span><br><span class="line">  count_res += count(path,symbol)</span><br></pre></td></tr></table></figure>



<p>(8) 多个path中 最长的path的数量</p>
<p>原本应该找出来多个path，但是，还是不太好找，还是直接计算path的长度吧～</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> urllib.parse <span class="keyword">import</span> urlparse</span><br><span class="line">url = <span class="string">&#x27;http://netloc/path;param?query=arg#frag&#x27;</span></span><br><span class="line">path = urlparse(url).path</span><br><span class="line">print(path)</span><br><span class="line">print(len(path))</span><br></pre></td></tr></table></figure>



<p>(9) 域名长度</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> urllib.parse <span class="keyword">import</span> urlparse</span><br><span class="line">url = <span class="string">&quot;https://bl-og.csdn.net/weixin_42793426/article?id=4/details.xyz.cn/88545939.xyz?id=3cdjdvjdfbv/csjhvufhgui?id=11&quot;</span></span><br><span class="line">domain = urlparse(url).netloc</span><br><span class="line">print(domain)</span><br><span class="line">print(len(domain))</span><br></pre></td></tr></table></figure>



<p>3、特征重要性</p>
<p>使用了三种算法，如特征相关、K-best和随机森林，来确定特征的重要性。</p>
<p>特征相关：Spearman和Pearson算法。1贡献性很大。</p>
<p>可以得出结论，路径中的分隔符、域中的分隔符、URL中的点数、URL的长度是其中最重要的特征。</p>
<p>4、特征预处理</p>
<p>删除无效的字符/数据值进行缩放/热编码/</p>
<p>5、机器学习算法：随机森林、KNN、LR、SVM。</p>
<p>对各个分类器赋予权值，投票得出最终结果。</p>
<p>但是目前到现在，还是不知道具体的要去如何实现。</p>
<p>用自己的数据库运行一下GitHub上的一个例子～</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">import</span> sys</span><br><span class="line"><span class="keyword">import</span> re</span><br><span class="line"><span class="keyword">import</span> matplotlib</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">from</span> os.path <span class="keyword">import</span> splitext</span><br><span class="line"><span class="keyword">import</span> ipaddress <span class="keyword">as</span> ip</span><br><span class="line"><span class="keyword">import</span> tldextract</span><br><span class="line"><span class="keyword">import</span> whois</span><br><span class="line"><span class="keyword">import</span> datetime</span><br><span class="line"><span class="keyword">from</span> urllib.parse <span class="keyword">import</span> urlparse</span><br><span class="line"></span><br><span class="line">df = pd.read_csv(<span class="string">&quot;10data.csv&quot;</span>)</span><br><span class="line">df = df.sample(frac=<span class="number">1</span>).reset_index(drop=<span class="literal">True</span>)	<span class="comment">#取样</span></span><br><span class="line">df.head()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">#2016&#x27;s top most suspicious TLD and words</span></span><br><span class="line">Suspicious_TLD=[<span class="string">&#x27;zip&#x27;</span>,<span class="string">&#x27;cricket&#x27;</span>,<span class="string">&#x27;link&#x27;</span>,<span class="string">&#x27;work&#x27;</span>,<span class="string">&#x27;party&#x27;</span>,<span class="string">&#x27;gq&#x27;</span>,<span class="string">&#x27;kim&#x27;</span>,<span class="string">&#x27;country&#x27;</span>,<span class="string">&#x27;science&#x27;</span>,<span class="string">&#x27;tk&#x27;</span>]</span><br><span class="line">Suspicious_Domain=[<span class="string">&#x27;luckytime.co.kr&#x27;</span>,<span class="string">&#x27;mattfoll.eu.interia.pl&#x27;</span>,<span class="string">&#x27;trafficholder.com&#x27;</span>,<span class="string">&#x27;dl.baixaki.com.br&#x27;</span>,<span class="string">&#x27;bembed.redtube.comr&#x27;</span>,<span class="string">&#x27;tags.expo9.exponential.com&#x27;</span>,<span class="string">&#x27;deepspacer.com&#x27;</span>,<span class="string">&#x27;funad.co.kr&#x27;</span>,<span class="string">&#x27;trafficconverter.biz&#x27;</span>]</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># Method to count number of dots</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">countdots</span>(<span class="params">url</span>):</span>  </span><br><span class="line">    <span class="keyword">return</span> url.count(<span class="string">&#x27;.&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Method to count number of delimeters</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">countdelim</span>(<span class="params">url</span>):</span></span><br><span class="line">    count = <span class="number">0</span></span><br><span class="line">    delim=[<span class="string">&#x27;;&#x27;</span>,<span class="string">&#x27;_&#x27;</span>,<span class="string">&#x27;?&#x27;</span>,<span class="string">&#x27;=&#x27;</span>,<span class="string">&#x27;&amp;&#x27;</span>]</span><br><span class="line">    <span class="keyword">for</span> each <span class="keyword">in</span> url:</span><br><span class="line">        <span class="keyword">if</span> each <span class="keyword">in</span> delim:</span><br><span class="line">            count = count + <span class="number">1</span></span><br><span class="line">    <span class="keyword">return</span> count</span><br><span class="line"></span><br><span class="line"><span class="comment"># Is IP addr present as th hostname, let&#x27;s validate</span></span><br><span class="line"><span class="keyword">import</span> ipaddress <span class="keyword">as</span> ip <span class="comment">#works only in python 3</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">isip</span>(<span class="params">uri</span>):</span></span><br><span class="line">    <span class="keyword">try</span>:</span><br><span class="line">        <span class="keyword">if</span> ip.ip_address(uri):</span><br><span class="line">            <span class="keyword">return</span> <span class="number">1</span></span><br><span class="line">    <span class="keyword">except</span>:</span><br><span class="line">        <span class="keyword">return</span> <span class="number">0</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#method to check the presence of hyphens</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">isPresentHyphen</span>(<span class="params">url</span>):</span></span><br><span class="line">    <span class="keyword">return</span> url.count(<span class="string">&#x27;-&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">isPresentAt</span>(<span class="params">url</span>):</span></span><br><span class="line">    <span class="keyword">return</span> url.count(<span class="string">&#x27;@&#x27;</span>)</span><br><span class="line">  </span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">isPresentDSlash</span>(<span class="params">url</span>):</span></span><br><span class="line">    <span class="keyword">return</span> url.count(<span class="string">&#x27;//&#x27;</span>)</span><br><span class="line">  </span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">countSubDir</span>(<span class="params">url</span>):</span></span><br><span class="line">    <span class="keyword">return</span> url.count(<span class="string">&#x27;/&#x27;</span>)</span><br><span class="line">  </span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_ext</span>(<span class="params">url</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;Return the filename extension from url, or &#x27;&#x27;.&quot;&quot;&quot;</span></span><br><span class="line">    root, ext = splitext(url)</span><br><span class="line">    <span class="keyword">return</span> ext</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">countSubDomain</span>(<span class="params">subdomain</span>):</span></span><br><span class="line">    <span class="keyword">if</span> <span class="keyword">not</span> subdomain:</span><br><span class="line">        <span class="keyword">return</span> <span class="number">0</span></span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="keyword">return</span> len(subdomain.split(<span class="string">&#x27;.&#x27;</span>))</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">countQueries</span>(<span class="params">query</span>):</span></span><br><span class="line">    <span class="keyword">if</span> <span class="keyword">not</span> query:</span><br><span class="line">        <span class="keyword">return</span> <span class="number">0</span></span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="keyword">return</span> len(query.split(<span class="string">&#x27;&amp;&#x27;</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment">#特征集设置</span></span><br><span class="line">featureSet = pd.DataFrame(columns=(<span class="string">&#x27;url&#x27;</span>,<span class="string">&#x27;no of dots&#x27;</span>,<span class="string">&#x27;presence of hyphen&#x27;</span>,<span class="string">&#x27;len of url&#x27;</span>,<span class="string">&#x27;presence of at&#x27;</span>,\</span><br><span class="line"><span class="string">&#x27;presence of double slash&#x27;</span>,<span class="string">&#x27;no of subdir&#x27;</span>,<span class="string">&#x27;no of subdomain&#x27;</span>,<span class="string">&#x27;len of domain&#x27;</span>,<span class="string">&#x27;no of queries&#x27;</span>,<span class="string">&#x27;is IP&#x27;</span>,<span class="string">&#x27;presence of Suspicious_TLD&#x27;</span>,\</span><br><span class="line"><span class="string">&#x27;presence of suspicious domain&#x27;</span>,<span class="string">&#x27;label&#x27;</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment">#XXX</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(len(df.head(<span class="number">20000</span>))):</span><br><span class="line">    features = getFeatures(df[<span class="string">&quot;url&quot;</span>].loc[i], df[<span class="string">&quot;label&quot;</span>].loc[i])    </span><br><span class="line">    featureSet.loc[i] = features</span><br><span class="line">    print(i)</span><br><span class="line"><span class="comment">#上面这一步是特征提取，对于2万条URL，在cronlab中处理需要3分20秒</span></span><br><span class="line">featureSet.head()</span><br><span class="line"></span><br></pre></td></tr></table></figure>



<p>特征的矩阵信息如下，感觉提取的有点问题。</p>
<p><img src="/2021/04/27/%E6%81%B6%E6%84%8FURL%E6%A3%80%E6%B5%8B%E4%B9%9D/4fig2.png" class="lazyload" data-srcset="/2021/04/27/%E6%81%B6%E6%84%8FURL%E6%A3%80%E6%B5%8B%E4%B9%9D/4fig2.png" srcset="data:image/png;base64,666" alt="4fig2"></p>
<p>下面就是测试的步骤了～</p>
<p>准确度说实话还是有点低了，而且假阳率过高，到70%左右了，可能是其中的步骤有问题了。</p>
<p><img src="/2021/04/27/%E6%81%B6%E6%84%8FURL%E6%A3%80%E6%B5%8B%E4%B9%9D/4fig3.png" class="lazyload" data-srcset="/2021/04/27/%E6%81%B6%E6%84%8FURL%E6%A3%80%E6%B5%8B%E4%B9%9D/4fig3.png" srcset="data:image/png;base64,666" alt="4fig3"></p>
<p>检查发现可能是特征提取有问题。如下图：</p>
<p><img src="/2021/04/27/%E6%81%B6%E6%84%8FURL%E6%A3%80%E6%B5%8B%E4%B9%9D/4fig4.png" class="lazyload" data-srcset="/2021/04/27/%E6%81%B6%E6%84%8FURL%E6%A3%80%E6%B5%8B%E4%B9%9D/4fig4.png" srcset="data:image/png;base64,666" alt="4fig4"></p>
<p>咋可能点数这么少呢～检查了一下，感觉数据集不是很好，没有前面的http://</p>
<p>使用桌面上的url data.csv文件进行测试，然后稍微更改了一些特征，让特征变得合理了一些～ 加入了TLD count</p>
<p>以下是用200个数据去测试。</p>
<p><img src="/2021/04/27/%E6%81%B6%E6%84%8FURL%E6%A3%80%E6%B5%8B%E4%B9%9D/4fig5.png" class="lazyload" data-srcset="/2021/04/27/%E6%81%B6%E6%84%8FURL%E6%A3%80%E6%B5%8B%E4%B9%9D/4fig5.png" srcset="data:image/png;base64,666" alt="4fig5"></p>
<p><img src="/2021/04/27/%E6%81%B6%E6%84%8FURL%E6%A3%80%E6%B5%8B%E4%B9%9D/4fig6.png" class="lazyload" data-srcset="/2021/04/27/%E6%81%B6%E6%84%8FURL%E6%A3%80%E6%B5%8B%E4%B9%9D/4fig6.png" srcset="data:image/png;base64,666" alt="4fig6"></p>
<p>准确度一下达到了0.9！！！从原来的0.89到了0.92！质的飞跃～</p>
<p><img src="/2021/04/27/%E6%81%B6%E6%84%8FURL%E6%A3%80%E6%B5%8B%E4%B9%9D/4fig7.png" class="lazyload" data-srcset="/2021/04/27/%E6%81%B6%E6%84%8FURL%E6%A3%80%E6%B5%8B%E4%B9%9D/4fig7.png" srcset="data:image/png;base64,666" alt="4fig7"></p>
<p><img src="/2021/04/27/%E6%81%B6%E6%84%8FURL%E6%A3%80%E6%B5%8B%E4%B9%9D/4fig8.png" class="lazyload" data-srcset="/2021/04/27/%E6%81%B6%E6%84%8FURL%E6%A3%80%E6%B5%8B%E4%B9%9D/4fig8.png" srcset="data:image/png;base64,666" alt="4fig8"></p>
<p>这结果也太好了叭！</p>
<p>然后用20000条数据去测试～</p>
<p><img src="/2021/04/27/%E6%81%B6%E6%84%8FURL%E6%A3%80%E6%B5%8B%E4%B9%9D/4fig9.png" class="lazyload" data-srcset="/2021/04/27/%E6%81%B6%E6%84%8FURL%E6%A3%80%E6%B5%8B%E4%B9%9D/4fig9.png" srcset="data:image/png;base64,666" alt="4fig9"></p>
<p>咋差别不大啊</p>
<p><img src="/2021/04/27/%E6%81%B6%E6%84%8FURL%E6%A3%80%E6%B5%8B%E4%B9%9D/4fig10.png" class="lazyload" data-srcset="/2021/04/27/%E6%81%B6%E6%84%8FURL%E6%A3%80%E6%B5%8B%E4%B9%9D/4fig10.png" srcset="data:image/png;base64,666" alt="4fig10"></p>
<p><img src="/2021/04/27/%E6%81%B6%E6%84%8FURL%E6%A3%80%E6%B5%8B%E4%B9%9D/4fig11.png" class="lazyload" data-srcset="/2021/04/27/%E6%81%B6%E6%84%8FURL%E6%A3%80%E6%B5%8B%E4%B9%9D/4fig11.png" srcset="data:image/png;base64,666" alt="4fig11"></p>
<p>咋和预期的不太一样呢，这不太好区分啊…</p>
<p>结果如下：</p>
<p><img src="/2021/04/27/%E6%81%B6%E6%84%8FURL%E6%A3%80%E6%B5%8B%E4%B9%9D/4fig12.png" class="lazyload" data-srcset="/2021/04/27/%E6%81%B6%E6%84%8FURL%E6%A3%80%E6%B5%8B%E4%B9%9D/4fig12.png" srcset="data:image/png;base64,666" alt="4fig12"></p>
<p><img src="/2021/04/27/%E6%81%B6%E6%84%8FURL%E6%A3%80%E6%B5%8B%E4%B9%9D/4fig13.png" class="lazyload" data-srcset="/2021/04/27/%E6%81%B6%E6%84%8FURL%E6%A3%80%E6%B5%8B%E4%B9%9D/4fig13.png" srcset="data:image/png;base64,666" alt="4fig13"></p>
<p>如果用所有的42万条数据去测试～大概要2h左右，时间有点久哦😯</p>
<p>预测结果可能不会改进太多吧，可能稍微提高一些</p>
<p>后续改进的时候可以加入如下特征：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">count_vowels</span>(<span class="params">text</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;Return the number of vowels.&quot;&quot;&quot;</span></span><br><span class="line">    vowels = [<span class="string">&#x27;a&#x27;</span>, <span class="string">&#x27;e&#x27;</span>, <span class="string">&#x27;i&#x27;</span>, <span class="string">&#x27;o&#x27;</span>, <span class="string">&#x27;u&#x27;</span>]</span><br><span class="line">    count = <span class="number">0</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> vowels:</span><br><span class="line">        count += text.lower().count(i)</span><br><span class="line">    <span class="keyword">return</span> count</span><br></pre></td></tr></table></figure>



<p>收获：</p>
<p>1、加入文中：为了逃避黑名单检测，攻击者可能对URL进行微小的修改。</p>
<p>2、加入文中：</p>
<p>URL格式</p>
<p><img src="/2021/04/27/%E6%81%B6%E6%84%8FURL%E6%A3%80%E6%B5%8B%E4%B9%9D/4fig14.png" class="lazyload" data-srcset="/2021/04/27/%E6%81%B6%E6%84%8FURL%E6%A3%80%E6%B5%8B%E4%B9%9D/4fig14.png" srcset="data:image/png;base64,666" alt="4fig14"></p>
<p>3、本篇比较好的地方可能是：实验结果较好 + 使用了特征评判 + 多个TLD</p>
<p>​    如果有多个TLD，如何真实的数量？上文中已解决。</p>
<h2 id="三十、从假新闻域中发现和度量恶意重定向活动"><a href="#三十、从假新闻域中发现和度量恶意重定向活动" class="headerlink" title="三十、从假新闻域中发现和度量恶意重定向活动"></a>三十、从假新闻域中发现和度量恶意重定向活动</h2><p>导师发的。</p>
<p>目标对象是：假新闻网站，重定向URL。</p>
<p>感觉和自己的方向不是很匹配，不再继续看了。</p>
<h2 id="三十一、Doc2Vec代码实现"><a href="#三十一、Doc2Vec代码实现" class="headerlink" title="三十一、Doc2Vec代码实现"></a>三十一、Doc2Vec代码实现</h2><p>看学长之前比赛的wp，学习到了Word2Vec的改进方法Doc2Vec，下面来实现一下，看看其效果。</p>
<p>之前测试的时候一直有错，现在来尝试一下别人的Word2Vec的实现。</p>
<p>链接如下：<a target="_blank" rel="noopener" href="https://github.com/ajinkyabhanudas/ML-in-network-security/blob/master/URL%20Classification%20using%20Word2Vec%20and%20FastText-Copy1.ipynb">https://github.com/ajinkyabhanudas/ML-in-network-security/blob/master/URL%20Classification%20using%20Word2Vec%20and%20FastText-Copy1.ipynb</a></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br><span class="line">193</span><br><span class="line">194</span><br><span class="line">195</span><br><span class="line">196</span><br><span class="line">197</span><br><span class="line">198</span><br><span class="line">199</span><br><span class="line">200</span><br><span class="line">201</span><br><span class="line">202</span><br><span class="line">203</span><br><span class="line">204</span><br><span class="line">205</span><br><span class="line">206</span><br><span class="line">207</span><br><span class="line">208</span><br><span class="line">209</span><br><span class="line">210</span><br><span class="line">211</span><br><span class="line">212</span><br><span class="line">213</span><br><span class="line">214</span><br><span class="line">215</span><br><span class="line">216</span><br><span class="line">217</span><br><span class="line">218</span><br><span class="line">219</span><br><span class="line">220</span><br><span class="line">221</span><br><span class="line">222</span><br><span class="line">223</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">import</span> keras</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">from</span> keras.models <span class="keyword">import</span> load_model</span><br><span class="line"><span class="keyword">from</span> urllib.parse <span class="keyword">import</span> urlparse,urlsplit</span><br><span class="line"><span class="keyword">import</span> gensim</span><br><span class="line"><span class="keyword">from</span> gensim.models <span class="keyword">import</span> Word2Vec, FastText</span><br><span class="line"><span class="keyword">from</span> multiprocessing.pool <span class="keyword">import</span> ThreadPool</span><br><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line">tf.get_logger().setLevel(<span class="string">&#x27;ERROR&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">splitter</span>(<span class="params">url_list</span>):</span></span><br><span class="line">    new_df=pd.DataFrame(columns=[<span class="string">&quot;scheme&quot;</span>,<span class="string">&quot;netloc&quot;</span>,<span class="string">&quot;path&quot;</span>,<span class="string">&quot;query&quot;</span>,<span class="string">&quot;fragment&quot;</span>])</span><br><span class="line">    <span class="keyword">for</span> url <span class="keyword">in</span> url_list:</span><br><span class="line">    <span class="comment">#     test = [urlsplit(url).scheme, urlsplit(url).netloc, urlsplit(url).path, urlsplit(url).query, urlsplit(url).fragment]</span></span><br><span class="line">        split_result=urlsplit(url)</span><br><span class="line">        scheme = split_result.scheme</span><br><span class="line">        netloc = split_result.netloc</span><br><span class="line">        path = split_result.path</span><br><span class="line">        query = split_result.query</span><br><span class="line">        new_df =  new_df.append(&#123;<span class="string">&#x27;scheme&#x27;</span>: scheme, <span class="string">&#x27;netloc&#x27;</span>: netloc, <span class="string">&#x27;path&#x27;</span>: path, <span class="string">&#x27;query&#x27;</span>: query&#125;, ignore_index= <span class="literal">True</span>)</span><br><span class="line">        new_df.fillna(<span class="number">0</span>,inplace=<span class="literal">True</span>)</span><br><span class="line">    <span class="keyword">return</span> (new_df)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">vectorizer_netloc</span>(<span class="params">new_df</span>):</span></span><br><span class="line">    </span><br><span class="line">    model1 = Word2Vec.load(<span class="string">&quot;w2v_url&quot;</span>)</span><br><span class="line">    </span><br><span class="line">    </span><br><span class="line">    splitr1= new_df[<span class="string">&quot;netloc&quot;</span>]</span><br><span class="line"></span><br><span class="line">    splitr1_1= [str(val).split(<span class="string">&quot;.&quot;</span>) <span class="keyword">for</span> val <span class="keyword">in</span> splitr1.tolist()]</span><br><span class="line">    </span><br><span class="line">    model1.build_vocab(splitr1_1, update=<span class="literal">True</span>)</span><br><span class="line">    model1.train(splitr1_1, total_examples=len(splitr1_1), epochs=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">    </span><br><span class="line">    holder1=[]</span><br><span class="line">    vec_df1= pd.DataFrame()</span><br><span class="line">    val=[<span class="number">0</span> <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">100</span>)]</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(len(splitr1_1)):</span><br><span class="line">        <span class="keyword">for</span> j <span class="keyword">in</span> range(len(splitr1_1[i])):</span><br><span class="line">            val += model1[splitr1_1[i][j]]</span><br><span class="line">        holder1.append(val.tolist())</span><br><span class="line">        val *=<span class="number">0</span></span><br><span class="line">    vec_df1=vec_df1.append(holder1)   </span><br><span class="line">    </span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span>(vec_df1)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">vectorizer_path</span>(<span class="params">new_df</span>):</span></span><br><span class="line">    </span><br><span class="line">    model2 = Word2Vec.load(<span class="string">&quot;w2v_path&quot;</span>)</span><br><span class="line">    </span><br><span class="line">    </span><br><span class="line">    splitr2= new_df[<span class="string">&quot;path&quot;</span>]</span><br><span class="line">    </span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    splitr2_1= [str(val).split(<span class="string">&quot;/&quot;</span>) <span class="keyword">for</span> val <span class="keyword">in</span> splitr2.tolist()]</span><br><span class="line">  </span><br><span class="line"></span><br><span class="line">    model2.build_vocab(splitr2_1, update=<span class="literal">True</span>)</span><br><span class="line">    model2.train(splitr2_1, total_examples=len(splitr2_1), epochs=<span class="number">1</span>)</span><br><span class="line">    </span><br><span class="line"></span><br><span class="line">    holder2=[]</span><br><span class="line">    vec_df2= pd.DataFrame()</span><br><span class="line">    val=[<span class="number">0</span> <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">100</span>)]</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(len(splitr2_1)):</span><br><span class="line">        <span class="keyword">for</span> j <span class="keyword">in</span> range(len(splitr2_1[i])):</span><br><span class="line">            val += model2[splitr2_1[i][j]]</span><br><span class="line">        holder2.append(val.tolist())</span><br><span class="line">        val *=<span class="number">0</span></span><br><span class="line">    vec_df2=vec_df2.append(holder2) </span><br><span class="line">    </span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span>(vec_df2)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">vectorizer_query</span>(<span class="params">new_df</span>):</span></span><br><span class="line">    </span><br><span class="line">    model3 = Word2Vec.load(<span class="string">&quot;w2v_query&quot;</span>)</span><br><span class="line">    </span><br><span class="line">    splitr3= new_df[<span class="string">&quot;query&quot;</span>]</span><br><span class="line"></span><br><span class="line">    splitr3_1= [str(val).split(<span class="string">&quot;=&quot;</span>) <span class="keyword">for</span> val <span class="keyword">in</span> splitr3.tolist()]</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    model3.build_vocab(splitr3_1, update=<span class="literal">True</span>)</span><br><span class="line">    model3.train(splitr3_1, total_examples=len(splitr3_1), epochs=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">    holder3=[]</span><br><span class="line">    vec_df3= pd.DataFrame()</span><br><span class="line">    val=[<span class="number">0</span> <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">100</span>)]</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(len(splitr3_1)):</span><br><span class="line">        <span class="keyword">for</span> j <span class="keyword">in</span> range(len(splitr3_1[i])):</span><br><span class="line">            val += model3[splitr3_1[i][j]]</span><br><span class="line">        holder3.append(val.tolist())</span><br><span class="line">        val *=<span class="number">0</span></span><br><span class="line">    vec_df3=vec_df3.append(holder3) </span><br><span class="line">    </span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span>(vec_df3)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">convert_to_df</span>(<span class="params">new_df</span>):</span></span><br><span class="line">    pool = ThreadPool(processes=<span class="number">2</span>)</span><br><span class="line">    new_df=new_df</span><br><span class="line">    async_result1 = pool.apply_async(vectorizer_netloc, args=(new_df,))</span><br><span class="line">    async_result2 = pool.apply_async(vectorizer_path, args=(new_df,))</span><br><span class="line">    async_result3 = pool.apply_async(vectorizer_query, args=(new_df,))<span class="comment"># tuple of args for foo</span></span><br><span class="line"></span><br><span class="line">    vec_df1 = async_result1.get()</span><br><span class="line">    vec_df2 = async_result2.get()</span><br><span class="line">    vec_df3 = async_result3.get()</span><br><span class="line">    </span><br><span class="line">    tester_df_batch=pd.DataFrame()</span><br><span class="line">    tester_df_batch=pd.concat([vec_df1,vec_df2,vec_df3], axis=<span class="number">1</span>)</span><br><span class="line">    <span class="keyword">return</span>(tester_df_batch)</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">return_avg_for_updated_url_score</span> (<span class="params">eps,model</span>):</span></span><br><span class="line">    switch = []</span><br><span class="line">    <span class="keyword">for</span> key <span class="keyword">in</span> eps.keys():</span><br><span class="line">        <span class="keyword">for</span> val <span class="keyword">in</span> range(len(eps[key])):</span><br><span class="line">            switch+=[(eps[key][val],key,calc_endpoint_score(model.predict(convert_to_df(splitter(eps[key]))).tolist()))]</span><br><span class="line"></span><br><span class="line">    y =pd.DataFrame(switch, columns=[<span class="string">&#x27;url&#x27;</span>,<span class="string">&#x27;ip&#x27;</span>,<span class="string">&#x27;eps_score&#x27;</span>])</span><br><span class="line"></span><br><span class="line">    grouped_sum = y.groupby(<span class="string">&#x27;url&#x27;</span>).sum()</span><br><span class="line">    grouped_size = y.groupby(<span class="string">&#x27;url&#x27;</span>).size()</span><br><span class="line">    <span class="comment"># grouped_size[&#x27;a&#x27;]</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    unique_urls = list(y[<span class="string">&#x27;url&#x27;</span>].unique())</span><br><span class="line">    avg_scores_of_endpoints_hitting_the_same_url =[]</span><br><span class="line">    <span class="keyword">for</span> url <span class="keyword">in</span> unique_urls :</span><br><span class="line">        avg_scores_of_endpoints_hitting_the_same_url += [&#123;url:grouped_sum[<span class="string">&#x27;eps_score&#x27;</span>][url]/grouped_size[url]&#125;]</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span>(avg_scores_of_endpoints_hitting_the_same_url,unique_urls)</span><br><span class="line">  </span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">calc_endpoint_score</span>(<span class="params">score</span>):</span></span><br><span class="line">    day_score = float()</span><br><span class="line">    bad_counter = int()</span><br><span class="line">    <span class="keyword">for</span> sc <span class="keyword">in</span> score:</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> sc[<span class="number">0</span>] &gt; <span class="number">0.5</span>:</span><br><span class="line">            day_score += sc[<span class="number">0</span>]</span><br><span class="line">            bad_counter += <span class="number">1</span></span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            day_score += sc[<span class="number">0</span>]</span><br><span class="line"></span><br><span class="line">    bad_counter</span><br><span class="line">    ep_w = (day_score/len(score))*(<span class="number">1.0</span>-(<span class="number">1</span>-(bad_counter/len(score))))</span><br><span class="line">    <span class="keyword">return</span>(ep_w)</span><br><span class="line">  </span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">vector_relatedness</span>(<span class="params">url</span>):</span></span><br><span class="line">    w_ = urlsplit(url)</span><br><span class="line">    w_netloc = w_.netloc.split(<span class="string">&#x27;.&#x27;</span>)</span><br><span class="line">    w_path = w_.path.split(<span class="string">&#x27;.&#x27;</span>)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">if</span>(w_netloc[<span class="number">0</span>]==<span class="string">&#x27;&#x27;</span>):</span><br><span class="line"></span><br><span class="line">        related_word=w_path</span><br><span class="line">        <span class="keyword">if</span>(related_word[<span class="number">0</span>] == <span class="string">&#x27;www&#x27;</span>):</span><br><span class="line">            related_word_finder = related_word[<span class="number">1</span>]</span><br><span class="line"></span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            related_word_finder = related_word[<span class="number">0</span>]</span><br><span class="line"></span><br><span class="line">    <span class="keyword">elif</span>(w_netloc[<span class="number">0</span>]==<span class="string">&#x27;www&#x27;</span>):</span><br><span class="line">        related_word_finder = w_netloc[<span class="number">1</span>]</span><br><span class="line"></span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        related_word_finder = w_netloc[<span class="number">0</span>]</span><br><span class="line">        </span><br><span class="line">        </span><br><span class="line">    FastText_model = FastText.load(<span class="string">&#x27;ft_.model&#x27;</span>)</span><br><span class="line">    relatedness_vector = FastText_model.wv.most_similar_cosmul(related_word_finder)</span><br><span class="line">    rv =np.mean([i[<span class="number">1</span>] <span class="keyword">for</span> i <span class="keyword">in</span> relatedness_vector])</span><br><span class="line">    <span class="keyword">return</span>(rv)</span><br><span class="line"></span><br><span class="line">  </span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">url_score_recalculator</span>(<span class="params">val_avg, unique_urls, url_scores</span>):</span></span><br><span class="line">    val_avg = val_avg</span><br><span class="line">    unique_urls = unique_urls</span><br><span class="line">    url_scores = [url_scores[i][<span class="number">0</span>] <span class="keyword">for</span> i <span class="keyword">in</span> range(len(url_scores))]</span><br><span class="line">    avg_list = [float(str(*list(val_avg[i].values()))) <span class="keyword">for</span> i <span class="keyword">in</span> range(len(val_avg))]</span><br><span class="line">    </span><br><span class="line">    rv =[vector_relatedness(url) <span class="keyword">for</span> url <span class="keyword">in</span> unique_urls]</span><br><span class="line">    m_rv_url_score = np.add(np.multiply(url_scores,<span class="number">0.85</span>),np.multiply(rv,<span class="number">0.1</span>))</span><br><span class="line">    </span><br><span class="line">    m_avg_list = np.multiply(avg_list,<span class="number">0.05</span>)</span><br><span class="line">    </span><br><span class="line">    updated_scores = np.subtract(m_rv_url_score,m_avg_list)</span><br><span class="line">    updated_url_score_dict = [&#123;unique_urls[i]:updated_scores[i]&#125; <span class="keyword">for</span> i <span class="keyword">in</span> range(len(unique_urls))]</span><br><span class="line">    <span class="keyword">return</span>(updated_url_score_dict)</span><br><span class="line"></span><br><span class="line">  </span><br><span class="line">eps = &#123;<span class="string">&#x27;192.168.6.6.1&#x27;</span>:[<span class="string">&quot;https://www.yagle.com/abcd/?=123&quot;</span>,<span class="string">&quot;https://www.google.com&quot;</span>,<span class="string">&quot;www.bitsadmin.com/&quot;</span>,<span class="string">&quot;https://www.yagle.com/abcd/?=123&quot;</span>,<span class="string">&quot;https://www.google.com&quot;</span>,<span class="string">&quot;www.bitsadmin.com/&quot;</span>,<span class="string">&quot;http://zastapiony.piklamp.nl/military.xbel?face=jlthEvD&amp;oil=70UfcEdppE&amp;similar=0T9GXVd&amp;plane=&amp;never=XzUI&amp;size=h-S&amp;building=3NHL3LH4j&amp;play=Ke9C4&amp;over=&amp;pattern=gKK&quot;</span>,<span class="string">&quot;https://www.google.com&quot;</span>],</span><br><span class="line">      <span class="string">&#x27;0.0.0.1&#x27;</span>:[<span class="string">&quot;https://www.yagle.com/abcd/?=123&quot;</span>,<span class="string">&quot;https://www.google.com&quot;</span>,<span class="string">&quot;www.bitsadmin.com/&quot;</span>]&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">### on assuming an input of the above format to the code</span></span><br><span class="line"></span><br><span class="line">model = load_model(<span class="string">&#x27;current.best__std.hdf5&#x27;</span>)</span><br><span class="line">val_avg, unique_urls = return_avg_for_updated_url_score(eps,model)</span><br><span class="line"></span><br><span class="line">final_vec = convert_to_df(splitter(eps[<span class="string">&#x27;192.168.6.6.1&#x27;</span>]))<span class="comment">##</span></span><br><span class="line">url_scores = model.predict(convert_to_df(splitter(unique_urls))).tolist()</span><br><span class="line">updated_url_score_dict = url_score_recalculator(val_avg, unique_urls, url_scores)</span><br><span class="line"></span><br><span class="line">score =  model.predict(final_vec).tolist()</span><br><span class="line">ep_sc = calc_endpoint_score(score)</span><br><span class="line"></span><br><span class="line">print(<span class="string">&quot;Endpoint Vectors for URLs of &#x27;192.168.6.6.1&#x27; (3 x 300) : &quot;</span>,final_vec)</span><br><span class="line">print(<span class="string">&quot;\nScores for all unique URLs not bound by the endpoint: \n&quot;</span>,url_scores)</span><br><span class="line">print(<span class="string">&quot;\nUpdated URL scores to dict: \n&quot;</span>,updated_url_score_dict)</span><br><span class="line">print(<span class="string">&quot;\nScores for URLs of &#x27;192.168.6.6.1&#x27; endpoint: \n&quot;</span>,score)</span><br><span class="line">print(<span class="string">&quot;\nEndpoint (&#x27;192.168.6.6.1&#x27;) score : &quot;</span>,ep_sc)</span><br><span class="line"></span><br></pre></td></tr></table></figure>



<p>代码如上，需要载入模型才能实验，并且模型比较大，就不测试了。代码看懂了，觉得比较有意义的部分是转换成矩阵的两个for循环。自己可能会借鉴到。</p>
<hr>
<p>下面看一个word2vec + CNN的实现，GitHub地址如右边，<a target="_blank" rel="noopener" href="https://github.com/cwellszhang/DetectMaliciousURL%EF%BC%8C%E5%87%86%E7%A1%AE%E5%BA%A6%E6%8D%AE%E8%AF%B4%E8%BE%BE%E5%88%B0%E4%BA%8696%%E3%80%82">https://github.com/cwellszhang/DetectMaliciousURL，准确度据说达到了96%。</a></p>
<p>参考文章：<a target="_blank" rel="noopener" href="https://blog.csdn.net/u011987514/article/details/71189491">https://blog.csdn.net/u011987514/article/details/71189491</a></p>
<p>我感觉如果把这个代码看懂了，其他不管多复杂的也就会了。这个应该是自己到目前为止见过最复杂的代码了～</p>
<p>由于需要使用tf的版本是1.0，所以在服务器中运行～</p>
<p>此时import pandas出现错误，参考 <a target="_blank" rel="noopener" href="https://docs.microsoft.com/en-us/answers/questions/55489/modulenotfounderror-no-module-named-39pandas39-whe.html%EF%BC%8C%E4%BD%BF%E7%94%A8%E8%AF%AD%E5%8F%A5%EF%BC%9Apython2">https://docs.microsoft.com/en-us/answers/questions/55489/modulenotfounderror-no-module-named-39pandas39-whe.html，使用语句：python2</a> -m pip install pandas</p>
<p>还需要安装gensim，<code>python2 -m pip install -U gensim</code>即可。</p>
<p>还有tf，<code>python2 -m pip install tensorflow==1.2.1</code></p>
<p>至此环境安装完成～</p>
<p>设置参数，用到了tf.flags.DEFINE_XXX，可以帮助我们添加命令行的可选参数。具体参考<a target="_blank" rel="noopener" href="https://blog.csdn.net/spring_willow/article/details/80111993">https://blog.csdn.net/spring_willow/article/details/80111993</a></p>
<p>对于FLAGS.replicas == False的情况，</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">if</span> FLAGS.replicas==<span class="literal">False</span>:</span><br><span class="line"> timestamp = str(int(time.time())) <span class="comment">#时间戳</span></span><br><span class="line"> out_dir = os.path.abspath(os.path.join(os.path.curdir, <span class="string">&quot;runs&quot;</span>, timestamp))<span class="comment">#用时间戳命名一个文件</span></span><br><span class="line"> print(<span class="string">&quot;Writing to &#123;&#125;\n&quot;</span>.format(out_dir))</span><br><span class="line"> <span class="keyword">if</span> <span class="keyword">not</span> os.path.exists(out_dir):</span><br><span class="line">    os.makedirs(out_dir)</span><br><span class="line"> <span class="comment"># Checkpoint directory. Tensorflow assumes this directory already exists so we need to create it</span></span><br><span class="line"> checkpoint_dir = os.path.abspath(os.path.join(out_dir, <span class="string">&quot;checkpoints&quot;</span>))</span><br><span class="line"> checkpoint_prefix = os.path.join(checkpoint_dir, <span class="string">&quot;model&quot;</span>)</span><br><span class="line"> <span class="keyword">if</span> <span class="keyword">not</span> os.path.exists(checkpoint_dir):</span><br><span class="line">        os.makedirs(checkpoint_dir)</span><br></pre></td></tr></table></figure>

<p>其实就是创建目录。</p>
<p>对于FLAGS.replicas == True的情况：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"> <span class="comment"># out_dir = os.path.abspath(os.path.join(os.path.curdir, &quot;runs&quot;, &quot;replicas&quot;))</span></span><br><span class="line"> out_dir = os.path.abspath(os.path.join(os.path.curdir,<span class="string">&quot;runs&quot;</span>,<span class="string">&quot;outputs&quot;</span>))</span><br><span class="line"> print(<span class="string">&quot;Writing to &#123;&#125;\n&quot;</span>.format(out_dir))</span><br><span class="line"> <span class="keyword">if</span> <span class="keyword">not</span> os.path.exists(out_dir):</span><br><span class="line">    os.makedirs(out_dir)</span><br><span class="line"><span class="comment">#创建out_dir文件。</span></span><br><span class="line">    </span><br><span class="line">    </span><br><span class="line"> checkpoint_dir = os.path.abspath(os.path.join(out_dir, <span class="string">&quot;checkpoints&quot;</span>))</span><br><span class="line"> checkpoint_prefix = os.path.join(checkpoint_dir, <span class="string">&quot;model&quot;</span>)</span><br><span class="line"> <span class="keyword">if</span> <span class="keyword">not</span> os.path.exists(checkpoint_dir):</span><br><span class="line">        os.makedirs(checkpoint_dir)</span><br><span class="line"> summary_dir= os.path.abspath(os.path.join(out_dir, <span class="string">&quot;summary&quot;</span>))</span><br><span class="line"> <span class="keyword">if</span> <span class="keyword">not</span> os.path.exists(summary_dir):</span><br><span class="line">        os.makedirs(summary_dir)</span><br><span class="line"> train_summary_dir = os.path.join(summary_dir,<span class="string">&quot;train&quot;</span>)</span><br><span class="line"> dev_summary_dir= os.path.join(summary_dir,<span class="string">&quot;dev&quot;</span>)</span><br><span class="line"> <span class="keyword">if</span> <span class="keyword">not</span> os.path.exists(train_summary_dir):</span><br><span class="line">        os.makedirs(train_summary_dir)</span><br><span class="line"> <span class="keyword">if</span> <span class="keyword">not</span> os.path.exists(dev_summary_dir):</span><br><span class="line">        os.makedirs(dev_summary_dir)</span><br></pre></td></tr></table></figure>

<p>也是创建目录</p>
<p>接下来定义了数据预处理</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">data_preprocess</span>():</span></span><br><span class="line">    <span class="comment"># Data preprocess</span></span><br><span class="line">    <span class="comment"># =======================================================</span></span><br><span class="line">    <span class="comment"># Load data</span></span><br><span class="line">    print(<span class="string">&quot;Loading data...&quot;</span>)</span><br><span class="line">    <span class="keyword">if</span> <span class="keyword">not</span> os.path.exists(os.path.join(out_dir,<span class="string">&quot;data_x.npy&quot;</span>)):</span><br><span class="line">          x, y = data_helper.load_data_and_labels(FLAGS.data_file)</span><br><span class="line">          <span class="comment"># Get embedding vector</span></span><br><span class="line">          x =x[:<span class="number">1000</span>]</span><br><span class="line">          y =y[:<span class="number">1000</span>]</span><br><span class="line">          sentences, max_document_length = data_helper.padding_sentences(x, <span class="string">&#x27;&lt;PADDING&gt;&#x27;</span>,padding_sentence_length=FLAGS.sequence_length)</span><br><span class="line">          print(len(sentences[<span class="number">0</span>]))</span><br><span class="line">          <span class="keyword">if</span> <span class="keyword">not</span> os.path.exists(os.path.join(out_dir,<span class="string">&quot;trained_word2vec.model&quot;</span>)):</span><br><span class="line">              x= np.array(word2vec_helpers.embedding_sentences(sentences, embedding_size = FLAGS.embedding_dim, file_to_save = os.path.join(out_dir, <span class="string">&#x27;trained_word2vec.model&#x27;</span>)))</span><br><span class="line">          <span class="keyword">else</span>:</span><br><span class="line">              print(<span class="string">&#x27;w2v model found...&#x27;</span>)</span><br><span class="line">              x = np.array(word2vec_helpers.embedding_sentences(sentences, embedding_size = FLAGS.embedding_dim, file_to_save = os.path.join(out_dir, <span class="string">&#x27;trained_word2vec.model&#x27;</span>),file_to_load=os.path.join(out_dir, <span class="string">&#x27;trained_word2vec.model&#x27;</span>)))</span><br><span class="line">          y = np.array(y)</span><br><span class="line">          <span class="comment"># np.save(os.path.join(out_dir,&quot;data_x.npy&quot;),x)</span></span><br><span class="line">          <span class="comment"># np.save(os.path.join(out_dir,&quot;data_y.npy&quot;),y)</span></span><br><span class="line">          <span class="keyword">del</span> sentences</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">          print(<span class="string">&#x27;data found...&#x27;</span>)</span><br><span class="line">          x= np.load(os.path.join(out_dir,<span class="string">&quot;data_x.npy&quot;</span>))</span><br><span class="line">          y= np.load(os.path.join(out_dir,<span class="string">&quot;data_y.npy&quot;</span>))</span><br><span class="line">    print(<span class="string">&quot;x.shape = &#123;&#125;&quot;</span>.format(x.shape))</span><br><span class="line">    print(<span class="string">&quot;y.shape = &#123;&#125;&quot;</span>.format(y.shape))</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Save params</span></span><br><span class="line">    <span class="keyword">if</span> <span class="keyword">not</span> os.path.exists(os.path.join(out_dir,<span class="string">&quot;training_params.pickle&quot;</span>)):</span><br><span class="line">        training_params_file = os.path.join(out_dir, <span class="string">&#x27;training_params.pickle&#x27;</span>)</span><br><span class="line">        params = &#123;<span class="string">&#x27;num_labels&#x27;</span> : FLAGS.num_labels, <span class="string">&#x27;max_document_length&#x27;</span> : max_document_length&#125;</span><br><span class="line">        data_helper.saveDict(params, training_params_file)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Shuffle data randomly</span></span><br><span class="line">    <span class="comment"># np.random.seed(10)</span></span><br><span class="line">    <span class="comment"># shuffle_indices = np.random.permutation(np.arange(len(y)))</span></span><br><span class="line">    <span class="comment"># x_shuffled = x[shuffle_indices]</span></span><br><span class="line">    <span class="comment"># y_shuffled = y[shuffle_indices]</span></span><br><span class="line">    <span class="comment"># del x,y</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># x_train, x_test, y_train, y_test = train_test_split(x_shuffled, y_shuffled, test_size=0.2, random_state=42)  # split into training and testing set 80/20 ratio</span></span><br><span class="line">    x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=<span class="number">0.2</span>, random_state=<span class="number">42</span>)  <span class="comment"># split into training and testing set 80/20 ratio</span></span><br><span class="line">    <span class="keyword">del</span> x,y</span><br><span class="line">    <span class="keyword">return</span> x_train, x_test, y_train, y_test</span><br></pre></td></tr></table></figure>

<p>读取数据：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">allurlscsv = pd.read_csv(path, <span class="string">&#x27;,&#x27;</span>, error_bad_lines=<span class="literal">False</span>)  <span class="comment"># reading file</span></span><br><span class="line">allurlsdata = pd.DataFrame(allurlscsv)  <span class="comment"># converting to a dataframe</span></span><br><span class="line">allurlsdata = np.array(allurlsdata)  <span class="comment"># converting it into an array</span></span><br><span class="line">random.shuffle(allurlsdata)</span><br></pre></td></tr></table></figure>

<p><img src="/2021/04/27/%E6%81%B6%E6%84%8FURL%E6%A3%80%E6%B5%8B%E4%B9%9D/6fig1.png" class="lazyload" data-srcset="/2021/04/27/%E6%81%B6%E6%84%8FURL%E6%A3%80%E6%B5%8B%E4%B9%9D/6fig1.png" srcset="data:image/png;base64,666" alt="6fig1"></p>
<p>把标签替换成数字：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">0</span>,len(y)):</span><br><span class="line">    <span class="keyword">if</span> y[i] ==<span class="string">&#x27;bad&#x27;</span>:</span><br><span class="line">        y[i]=<span class="number">0</span></span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        y[i]=<span class="number">1</span></span><br></pre></td></tr></table></figure>

<p>但是函数to_categorical好像有点问题，标签读取的都是1。但是完整测试的时候好像又正常了～</p>
<p>下面padding_sentences函数，用于填充或者切除一定的长度。</p>
<p>下面看模型是否存在。如果不存在：调用embedding_sentences，嵌入句子。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">if</span> <span class="keyword">not</span> os.path.exists(os.path.join(out_dir,<span class="string">&quot;trained_word2vec.model&quot;</span>)):</span><br><span class="line">  x= np.array(word2vec_helpers.embedding_sentences(sentences, embedding_size = FLAGS.embedding_dim, file_to_save = os.path.join(out_dir, <span class="string">&#x27;trained_word2vec.model&#x27;</span>)))</span><br></pre></td></tr></table></figure>

<p>如果存在，也是调用embedding_sentences，略微有些不同。</p>
<p>如果有data_x.npy,直接载入数据。</p>
<p>下面保存参数，可以置乱一下，但是文中后来删去了～然后划分为训练集和测试集。</p>
<p>然后判断，如果replicas == True，执行下面的操作。</p>
<p>主要就是预处理和CNN。CNN具体的结构比较复杂，现在就不看啦～～</p>
<hr>
<p>看评价的代码</p>
<p>前面的主要就是加载参数，然后还是读取数据，词嵌入。后面的准确度是自己手动输入的～</p>
<p>代码看完了，比自己想象的要简单一些。</p>
<p>下面自己写一个word2vec + 传统机器学习的算法～</p>
<hr>
<p>啊啊啊啊啊！原本word2vec嵌入之后维度还是有一点点的问题，使用语句<code>X = X.reshape(X.shape[0],-1)</code>，将三维的转为二维。</p>
<p>此时参数设置：max_sequence_length = 10，embedding_dim = 12</p>
<p>使用很少的几百条数据进行测试～模型使用LR，结果如下</p>
<p><img src="/2021/04/27/%E6%81%B6%E6%84%8FURL%E6%A3%80%E6%B5%8B%E4%B9%9D/6fig2.png" class="lazyload" data-srcset="/2021/04/27/%E6%81%B6%E6%84%8FURL%E6%A3%80%E6%B5%8B%E4%B9%9D/6fig2.png" srcset="data:image/png;base64,666" alt="6fig2"></p>
<p>多用一些数据！</p>
<p>LR，结果如下：</p>
<p><img src="/2021/04/27/%E6%81%B6%E6%84%8FURL%E6%A3%80%E6%B5%8B%E4%B9%9D/6fig3.png" class="lazyload" data-srcset="/2021/04/27/%E6%81%B6%E6%84%8FURL%E6%A3%80%E6%B5%8B%E4%B9%9D/6fig3.png" srcset="data:image/png;base64,666" alt="6fig3"></p>
<p>如果使用决策树，结果如下：</p>
<p><img src="/2021/04/27/%E6%81%B6%E6%84%8FURL%E6%A3%80%E6%B5%8B%E4%B9%9D/6fig4.png" class="lazyload" data-srcset="/2021/04/27/%E6%81%B6%E6%84%8FURL%E6%A3%80%E6%B5%8B%E4%B9%9D/6fig4.png" srcset="data:image/png;base64,666" alt="6fig4"></p>
<p>啊啊啊 啊啊啊 啊结果这么好！决策树我爱你！</p>
<p>再使用随机森林，嗷嗷啊怎么会这么好？我开始怀疑自己了</p>
<p><img src="/2021/04/27/%E6%81%B6%E6%84%8FURL%E6%A3%80%E6%B5%8B%E4%B9%9D/6fig5.png" class="lazyload" data-srcset="/2021/04/27/%E6%81%B6%E6%84%8FURL%E6%A3%80%E6%B5%8B%E4%B9%9D/6fig5.png" srcset="data:image/png;base64,666" alt="6fig5"></p>
<p>变换一下参数：max_sequence_length = 15，embedding_dim = 12</p>
<p>LR：直接killed了，可能是参数设置有点大了。</p>
<p>决策树：略略有些下降～</p>
<p><img src="/2021/04/27/%E6%81%B6%E6%84%8FURL%E6%A3%80%E6%B5%8B%E4%B9%9D/6fig6.png" class="lazyload" data-srcset="/2021/04/27/%E6%81%B6%E6%84%8FURL%E6%A3%80%E6%B5%8B%E4%B9%9D/6fig6.png" srcset="data:image/png;base64,666" alt="6fig6"></p>
<p>随机森林，准确度略有提升～</p>
<p><img src="/2021/04/27/%E6%81%B6%E6%84%8FURL%E6%A3%80%E6%B5%8B%E4%B9%9D/6fig7.png" class="lazyload" data-srcset="/2021/04/27/%E6%81%B6%E6%84%8FURL%E6%A3%80%E6%B5%8B%E4%B9%9D/6fig7.png" srcset="data:image/png;base64,666" alt="6fig7"></p>
<p>再变换一下参数：max_sequence_length = 12，embedding_dim = 12</p>
<p>LR：稍微有一点点提升～</p>
<p><img src="/2021/04/27/%E6%81%B6%E6%84%8FURL%E6%A3%80%E6%B5%8B%E4%B9%9D/6fig8.png" class="lazyload" data-srcset="/2021/04/27/%E6%81%B6%E6%84%8FURL%E6%A3%80%E6%B5%8B%E4%B9%9D/6fig8.png" srcset="data:image/png;base64,666" alt="6fig8"></p>
<p>决策树：有一点点点小提升</p>
<p><img src="/2021/04/27/%E6%81%B6%E6%84%8FURL%E6%A3%80%E6%B5%8B%E4%B9%9D/6fig9.png" class="lazyload" data-srcset="/2021/04/27/%E6%81%B6%E6%84%8FURL%E6%A3%80%E6%B5%8B%E4%B9%9D/6fig9.png" srcset="data:image/png;base64,666" alt="6fig9"></p>
<p>随机森林：</p>
<p><img src="/2021/04/27/%E6%81%B6%E6%84%8FURL%E6%A3%80%E6%B5%8B%E4%B9%9D/6fig10.png" class="lazyload" data-srcset="/2021/04/27/%E6%81%B6%E6%84%8FURL%E6%A3%80%E6%B5%8B%E4%B9%9D/6fig10.png" srcset="data:image/png;base64,666" alt="6fig10"></p>
<p>矩阵变换那里尝试一下 一个url的矢量变成一维</p>
<p>X = X.reshape(len(X),max_sequence_length*embedding_dim)</p>
<p>其实没太大的差别啦～随机森林如下：</p>
<p><img src="/2021/04/27/%E6%81%B6%E6%84%8FURL%E6%A3%80%E6%B5%8B%E4%B9%9D/6fig11.png" class="lazyload" data-srcset="/2021/04/27/%E6%81%B6%E6%84%8FURL%E6%A3%80%E6%B5%8B%E4%B9%9D/6fig11.png" srcset="data:image/png;base64,666" alt="6fig11"></p>
<p>使用Doc2vev</p>
<p>得到错误：’list’ object has no attribute ‘words’，这是因为word后面需要加上标签，标号。</p>
<p>参考 <a target="_blank" rel="noopener" href="https://github.com/Microstrong0305/WeChat-zhihu-csdnblog-code/blob/master/NLP/Doc2vec/Doc2vec.py">https://github.com/Microstrong0305/WeChat-zhihu-csdnblog-code/blob/master/NLP/Doc2vec/Doc2vec.py</a></p>
<p>不太清楚标签那边是怎么弄的。</p>
<p>先用 tagged_data = [TaggedDocument(words=_d, tags=[str(y[i])]) for i, _d in enumerate(sentences)]，标签是y的标签</p>
<p>使用随机森林，啊这，模型的准确度有点过高</p>
<p><img src="/2021/04/27/%E6%81%B6%E6%84%8FURL%E6%A3%80%E6%B5%8B%E4%B9%9D/6fig12.png" class="lazyload" data-srcset="/2021/04/27/%E6%81%B6%E6%84%8FURL%E6%A3%80%E6%B5%8B%E4%B9%9D/6fig12.png" srcset="data:image/png;base64,666" alt="6fig12"></p>
<p>如果标签使用tagged_data = [TaggedDocument(words=_d, tags=[str(i)]) <strong>for</strong> i, _d <strong>in</strong> enumerate(sentences)]</p>
<p>数据好的有点可怕～</p>
<p>随机森林</p>
<p><img src="/2021/04/27/%E6%81%B6%E6%84%8FURL%E6%A3%80%E6%B5%8B%E4%B9%9D/6fig13.png" class="lazyload" data-srcset="/2021/04/27/%E6%81%B6%E6%84%8FURL%E6%A3%80%E6%B5%8B%E4%B9%9D/6fig13.png" srcset="data:image/png;base64,666" alt="6fig13"></p>
<p>决策树</p>
<p><img src="/2021/04/27/%E6%81%B6%E6%84%8FURL%E6%A3%80%E6%B5%8B%E4%B9%9D/6fig14.png" class="lazyload" data-srcset="/2021/04/27/%E6%81%B6%E6%84%8FURL%E6%A3%80%E6%B5%8B%E4%B9%9D/6fig14.png" srcset="data:image/png;base64,666" alt="6fig14"></p>
<p>如果使用逻辑回归，效果还是有一丝丝的差～</p>
<p><img src="/2021/04/27/%E6%81%B6%E6%84%8FURL%E6%A3%80%E6%B5%8B%E4%B9%9D/6fig15.png" class="lazyload" data-srcset="/2021/04/27/%E6%81%B6%E6%84%8FURL%E6%A3%80%E6%B5%8B%E4%B9%9D/6fig15.png" srcset="data:image/png;base64,666" alt="6fig15"></p>
<p>下面是之前参考过的dord2vec，重点看人家的标签的部分</p>
<p>测试代码如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#先安装</span></span><br><span class="line">pip install gensim</span><br><span class="line"></span><br><span class="line"><span class="comment">#ipython中如下操作</span></span><br><span class="line"><span class="keyword">import</span> sys</span><br><span class="line"><span class="keyword">import</span> logging</span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">import</span> gensim</span><br><span class="line"></span><br><span class="line"><span class="comment"># 引入doc2vec</span></span><br><span class="line"><span class="keyword">from</span> gensim.models <span class="keyword">import</span> Doc2Vec</span><br><span class="line">curPath = os.path.abspath(os.path.dirname(<span class="string">&#x27;10.csv&#x27;</span>))</span><br><span class="line">rootPath = os.path.split(curPath)[<span class="number">0</span>]</span><br><span class="line">sys.path.append(rootPath)</span><br><span class="line"><span class="comment">#from utilties import ko_title2words</span></span><br><span class="line">logging.basicConfig(format=<span class="string">&#x27;%(asctime)s : %(levelname)s : %(message)s&#x27;</span>, level=logging.INFO)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 加载数据</span></span><br><span class="line">documents = []</span><br><span class="line"><span class="comment"># 使用count当做每个句子的“标签”，标签和每个句子是一一对应的</span></span><br><span class="line">count = <span class="number">0</span></span><br><span class="line"><span class="keyword">with</span> open(<span class="string">&#x27;../data/titles/ko.video.corpus&#x27;</span>,<span class="string">&#x27;r&#x27;</span>) <span class="keyword">as</span> f:</span><br><span class="line">    <span class="keyword">for</span> line <span class="keyword">in</span> f:</span><br><span class="line">        title = unicode(line, <span class="string">&#x27;utf-8&#x27;</span>)</span><br><span class="line">        <span class="comment"># 切词，返回的结果是列表类型</span></span><br><span class="line">        words = ko_title2words(title)</span><br><span class="line">        <span class="comment"># 这里documents里的每个元素是二元组，具体可以查看函数文档</span></span><br><span class="line">        documents.append(gensim.models.doc2vec.TaggedDocument(words, [str(count)]))</span><br><span class="line">        count += <span class="number">1</span></span><br><span class="line">        <span class="keyword">if</span> count % <span class="number">10000</span> == <span class="number">0</span>:</span><br><span class="line">            logging.info(<span class="string">&#x27;&#123;&#125; has loaded...&#x27;</span>.format(count))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 模型训练</span></span><br><span class="line">model = Doc2Vec(documents, dm=<span class="number">1</span>, size=<span class="number">100</span>, window=<span class="number">8</span>, min_count=<span class="number">5</span>, workers=<span class="number">4</span>)</span><br><span class="line"><span class="comment"># 保存模型</span></span><br><span class="line">model.save(<span class="string">&#x27;models/ko_d2v.model&#x27;</span>)</span><br><span class="line">curPath</span><br></pre></td></tr></table></figure>



<p>先看没有预训练的，代码如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">import</span> gensim.models <span class="keyword">as</span> g</span><br><span class="line"><span class="keyword">import</span> logging</span><br><span class="line"></span><br><span class="line">data = pd.read_csv(<span class="string">&quot;10data.csv&quot;</span>)</span><br><span class="line">y = data[<span class="string">&quot;label&quot;</span>]</span><br><span class="line">url_list = data[<span class="string">&quot;url&quot;</span>]</span><br><span class="line"></span><br><span class="line">vector_size = <span class="number">300</span></span><br><span class="line">window_size = <span class="number">15</span></span><br><span class="line">min_count = <span class="number">1</span></span><br><span class="line">sampling_threshold = <span class="number">1e-5</span></span><br><span class="line">negative_size = <span class="number">5</span></span><br><span class="line">train_epoch = <span class="number">100</span></span><br><span class="line">dm = <span class="number">0</span> <span class="comment">#0 = dbow; 1 = dmpv</span></span><br><span class="line">worker_count = <span class="number">1</span> <span class="comment">#number of parallel processes</span></span><br><span class="line">logging.basicConfig(format=<span class="string">&#x27;%(asctime)s : %(levelname)s : %(message)s&#x27;</span>, level=logging.INFO)</span><br><span class="line"></span><br><span class="line">docs = g.doc2vec.TaggedLineDocument(url_list)</span><br><span class="line">trans_vector = g.Doc2Vec(docs, size=vector_size, window=window_size, min_count=min_count, sample=sampling_threshold, workers=worker_count, hs=<span class="number">0</span>, dm=dm, negative=negative_size, dbow_words=<span class="number">1</span>, dm_concat=<span class="number">1</span>, iter=train_epoch)</span><br></pre></td></tr></table></figure>

<p>出现如下报错</p>
<p><img src="/2021/04/27/%E6%81%B6%E6%84%8FURL%E6%A3%80%E6%B5%8B%E4%B9%9D/6fig16.png" class="lazyload" data-srcset="/2021/04/27/%E6%81%B6%E6%84%8FURL%E6%A3%80%E6%B5%8B%E4%B9%9D/6fig16.png" srcset="data:image/png;base64,666" alt="6fig16"></p>
<p>因为TaggedLineDocument的参数需要时文件的地址。 </p>
<p>此处需要将url进行utf-8编码，然后合在一起。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">with</span> open(<span class="string">&#x27;../data/titles/ko.video.corpus&#x27;</span>,<span class="string">&#x27;r&#x27;</span>) <span class="keyword">as</span> f:</span><br><span class="line">    <span class="keyword">for</span> line <span class="keyword">in</span> f:</span><br><span class="line">        title = unicode(line, <span class="string">&#x27;utf-8&#x27;</span>)</span><br><span class="line">        <span class="comment"># 切词，返回的结果是列表类型</span></span><br><span class="line">        words = ko_title2words(title)</span><br><span class="line">        <span class="comment"># 这里documents里的每个元素是二元组，具体可以查看函数文档</span></span><br><span class="line">        documents.append(gensim.models.doc2vec.TaggedDocument(words, [str(count)]))</span><br><span class="line">        count += <span class="number">1</span></span><br><span class="line">        <span class="keyword">if</span> count % <span class="number">10000</span> == <span class="number">0</span>:</span><br><span class="line">            logging.info(<span class="string">&#x27;&#123;&#125; has loaded...&#x27;</span>.format(count))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 模型训练</span></span><br><span class="line">model = Doc2Vec(documents, dm=<span class="number">1</span>, size=<span class="number">100</span>, window=<span class="number">8</span>, min_count=<span class="number">5</span>, workers=<span class="number">4</span>)</span><br></pre></td></tr></table></figure>



<p>到现在也还是没有实现…</p>
<p>先实现一个简单的～关于新闻的</p>
<p>停词表参考 <a target="_blank" rel="noopener" href="https://github.com/goto456/stopwords">https://github.com/goto456/stopwords</a> 使用四川大学的。</p>
<p>代码实现如下</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> gensim <span class="keyword">import</span> models,corpora,similarities</span><br><span class="line"><span class="keyword">import</span> jieba.posseg <span class="keyword">as</span> pseg</span><br><span class="line"><span class="keyword">from</span> gensim.models.doc2vec <span class="keyword">import</span> TaggedDocument,Doc2Vec</span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">a_sub_b</span>(<span class="params">a,b</span>):</span></span><br><span class="line">    ret = []</span><br><span class="line">    <span class="keyword">for</span> el <span class="keyword">in</span> a:</span><br><span class="line">        <span class="keyword">if</span> el <span class="keyword">not</span> <span class="keyword">in</span> b:</span><br><span class="line">            ret.append(el)</span><br><span class="line">    <span class="keyword">return</span> ret</span><br><span class="line">stop = [line.strip().encode().decode(<span class="string">&#x27;utf-8&#x27;</span>) <span class="keyword">for</span> line <span class="keyword">in</span> open(<span class="string">&#x27;stopwords.txt&#x27;</span>).readlines() ]</span><br></pre></td></tr></table></figure>



<p>stop是一个list，内容如下：</p>
<p><img src="/2021/04/27/%E6%81%B6%E6%84%8FURL%E6%A3%80%E6%B5%8B%E4%B9%9D/6fig17.png" class="lazyload" data-srcset="/2021/04/27/%E6%81%B6%E6%84%8FURL%E6%A3%80%E6%B5%8B%E4%B9%9D/6fig17.png" srcset="data:image/png;base64,666" alt="6fig17"></p>
<p>然后还是有很多错误啊啊啊啊啊，主要是格式问题，不改了气死了！</p>
<p>自己也做钓鱼检测！！</p>
<p>再测试一下新的数据集，UNC大学2016年的URL，看看自己的效果是不是真的那么好</p>
<p>是的！</p>
<p>我他妈的都有点害怕，到底是不是过拟合了啊哈哈哈哈，要是都话那我真他妈完蛋了。</p>
<p>最后决定使用<a target="_blank" rel="noopener" href="https://www.kaggle.com/siddharthkumar25/malicious-and-benign-urls/tasks%E7%9A%84%E6%95%B0%E6%8D%AE%E9%9B%86%E8%BF%9B%E8%A1%8C%E6%B5%8B%E8%AF%95">https://www.kaggle.com/siddharthkumar25/malicious-and-benign-urls/tasks的数据集进行测试</a></p>
<p>收获：</p>
<p>1、文中可以加入如下语句：</p>
<p>一般钓鱼的链接会在域名和主机名之间作文章，进行一些域名混淆的恶意行为</p>
<p>而恶意用户请求会从请求参数作文章，比如进行恶意SQL注入</p>
<p>2、突发奇想：能不能构建两个库，一个良性一个恶意，最后看查询的URL和哪个的相似度更高？</p>
<p>结论：这个和有监督学习其实就是一个思想。</p>
<p>3、各种方法比较</p>
<p><img src="/2021/04/27/%E6%81%B6%E6%84%8FURL%E6%A3%80%E6%B5%8B%E4%B9%9D/6fig18.png" class="lazyload" data-srcset="/2021/04/27/%E6%81%B6%E6%84%8FURL%E6%A3%80%E6%B5%8B%E4%B9%9D/6fig18.png" srcset="data:image/png;base64,666" alt="6fig18"></p>
<p><img src="/2021/04/27/%E6%81%B6%E6%84%8FURL%E6%A3%80%E6%B5%8B%E4%B9%9D/6fig19.png" class="lazyload" data-srcset="/2021/04/27/%E6%81%B6%E6%84%8FURL%E6%A3%80%E6%B5%8B%E4%B9%9D/6fig19.png" srcset="data:image/png;base64,666" alt="6fig19"></p>
<p>代码实现过程参考 <a target="_blank" rel="noopener" href="https://github.com/Microstrong0305/WeChat-zhihu-csdnblog-code/blob/master/NLP/Doc2vec/Doc2vec.py">https://github.com/Microstrong0305/WeChat-zhihu-csdnblog-code/blob/master/NLP/Doc2vec/Doc2vec.py</a></p>

  
  
    
    <div class='footer'>
      
      
      
        <div class='copyright'>
          <blockquote>
            
              
                <p>博客内容遵循 署名-非商业性使用-相同方式共享 4.0 国际 (CC BY-NC-SA 4.0) 协议</p>

              
            
              
                <p>本文永久链接是：<a href=http://example.com/2021/04/27/%E6%81%B6%E6%84%8FURL%E6%A3%80%E6%B5%8B%E4%B9%9D/>http://example.com/2021/04/27/%E6%81%B6%E6%84%8FURL%E6%A3%80%E6%B5%8B%E4%B9%9D/</a></p>
              
            
          </blockquote>
        </div>
      
      
    </div>
  
  
    


  <div class='article-meta' id="bottom">
    <div class='new-meta-box'>
      
        
          <div class="new-meta-item date" itemprop="dateUpdated" datetime="2021-05-25T19:27:21+08:00">
  <a class='notlink'>
    <i class="fas fa-edit fa-fw" aria-hidden="true"></i>
    <p>更新于：May 25, 2021</p>
  </a>
</div>

        
      
        
          

        
      
        
          
  <div class="new-meta-item share -mob-share-list">
  <div class="-mob-share-list share-body">
    
      
        <a class="-mob-share-qq" title="" rel="external nofollow noopener noreferrer noopener"
          
          target="_blank" href="http://connect.qq.com/widget/shareqq/index.html?url=http://example.com/2021/04/27/%E6%81%B6%E6%84%8FURL%E6%A3%80%E6%B5%8B%E4%B9%9D/&title=恶意URL检测(九)一些比较杂的文章 - 虽然没什么用但还是想记下来&summary="
          
          >
          
            <img src="https://cdn.jsdelivr.net/gh/volantis-x/cdn-org/logo/128/qq.png" class="lazyload" data-srcset="https://cdn.jsdelivr.net/gh/volantis-x/cdn-org/logo/128/qq.png" srcset="data:image/png;base64,666">
          
        </a>
      
    
      
        <a class="-mob-share-qzone" title="" rel="external nofollow noopener noreferrer noopener"
          
          target="_blank" href="https://sns.qzone.qq.com/cgi-bin/qzshare/cgi_qzshare_onekey?url=http://example.com/2021/04/27/%E6%81%B6%E6%84%8FURL%E6%A3%80%E6%B5%8B%E4%B9%9D/&title=恶意URL检测(九)一些比较杂的文章 - 虽然没什么用但还是想记下来&summary="
          
          >
          
            <img src="https://cdn.jsdelivr.net/gh/volantis-x/cdn-org/logo/128/qzone.png" class="lazyload" data-srcset="https://cdn.jsdelivr.net/gh/volantis-x/cdn-org/logo/128/qzone.png" srcset="data:image/png;base64,666">
          
        </a>
      
    
      
        <a class="-mob-share-weibo" title="" rel="external nofollow noopener noreferrer noopener"
          
          target="_blank" href="http://service.weibo.com/share/share.php?url=http://example.com/2021/04/27/%E6%81%B6%E6%84%8FURL%E6%A3%80%E6%B5%8B%E4%B9%9D/&title=恶意URL检测(九)一些比较杂的文章 - 虽然没什么用但还是想记下来&summary="
          
          >
          
            <img src="https://cdn.jsdelivr.net/gh/volantis-x/cdn-org/logo/128/weibo.png" class="lazyload" data-srcset="https://cdn.jsdelivr.net/gh/volantis-x/cdn-org/logo/128/weibo.png" srcset="data:image/png;base64,666">
          
        </a>
      
    
      
    
      
    
  </div>
</div>



        
      
    </div>
  </div>


  
  

  
    <div class="prev-next">
      
        <a class='prev' href='/2021/04/28/%E5%A0%86%E5%88%A9%E7%94%A8%E4%B8%89%E4%B9%8Boffbyone/'>
          <p class='title'><i class="fas fa-chevron-left" aria-hidden="true"></i>堆利用(三)之offbyone</p>
          <p class='content'>本文主要记录阅读一些文章时的笔记及总结
一、Asis CTF 2016 b00ks wp (null off-by-one)【详】文章地址 https://www.jianshu.com/p/5...</p>
        </a>
      
      
        <a class='next' href='/2021/04/26/TPlinkSR20%E8%BF%9C%E7%A8%8B%E4%BB%A3%E7%A0%81%E6%89%A7%E8%A1%8C%E6%BC%8F%E6%B4%9E/'>
          <p class='title'>TPlink SR20远程代码执行漏洞<i class="fas fa-chevron-right" aria-hidden="true"></i></p>
          <p class='content'>漏洞公布时间：2019年3月26日
漏洞原理参考 https://paper.seebug.org/879/#_5
环境搭建1、ubuntu 16.04
2、Qemu 2.5.0
1234567...</p>
        </a>
      
    </div>
  
</article>


  

  <article class="post white-box reveal shadow" id="comments">
    <p ct><i class='fas fa-comments'></i> 评论</p>
    
    <div id="valine_container" class="valine_thread">
  <i class="fas fa-cog fa-spin fa-fw fa-2x"></i>
</div>

  </article>






</div>
<aside class='l_side'>
  
  
    
    



  <section class="widget toc-wrapper shadow desktop mobile" id="toc-div" >
    
  <header>
    
      <i class="fas fa-list fa-fw" aria-hidden="true"></i><span class='name'>本文目录</span>
    
  </header>


    <div class='content'>
        <ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%BA%8C%E5%8D%81%E5%9B%9B%E3%80%81%E8%AE%AD%E7%BB%83Transformers"><span class="toc-text">二十四、训练Transformers</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%BA%8C%E5%8D%81%E4%BA%94%E3%80%81%E5%8C%BA%E5%9D%97%E9%93%BE-%E9%92%93%E9%B1%BC%E6%A3%80%E6%B5%8B"><span class="toc-text">二十五、区块链 钓鱼检测</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%BA%8C%E5%8D%81%E5%85%AD%E3%80%81XLNet%EF%BC%9A%E8%AF%AD%E8%A8%80%E7%90%86%E8%A7%A3%E7%9A%84%E5%B9%BF%E4%B9%89%E5%9B%9E%E5%BD%92%E9%A2%84%E8%AE%AD%E7%BB%83"><span class="toc-text">二十六、XLNet：语言理解的广义回归预训练</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%BA%8C%E5%8D%81%E4%B8%83%E3%80%81Universal-Sentence-Encoder-%E9%80%9A%E7%94%A8%E5%8F%A5%E5%AD%90%E7%BC%96%E7%A0%81%E5%99%A8"><span class="toc-text">二十七、Universal Sentence Encoder 通用句子编码器</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%BA%8C%E5%8D%81%E5%85%AB%E3%80%81%E9%80%9A%E8%BF%87%E7%BD%91%E9%A1%B5%E4%B8%AD%E7%9A%84-6-%E4%B8%AA%E7%89%B9%E5%BE%81%E5%AD%97%E6%AE%B5%E6%A3%80%E6%B5%8B%E9%92%93%E9%B1%BC%E7%BD%91%E7%AB%99"><span class="toc-text">二十八、通过网页中的 6 个特征字段检测钓鱼网站</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%BA%8C%E5%8D%81%E4%B9%9D%E3%80%81%E4%BD%BF%E7%94%A8%E8%AF%8D%E6%B1%87%E7%89%B9%E5%BE%81%E6%A3%80%E6%B5%8B%E9%92%93%E9%B1%BC%E7%BD%91%E7%AB%99"><span class="toc-text">二十九、使用词汇特征检测钓鱼网站</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%B8%89%E5%8D%81%E3%80%81%E4%BB%8E%E5%81%87%E6%96%B0%E9%97%BB%E5%9F%9F%E4%B8%AD%E5%8F%91%E7%8E%B0%E5%92%8C%E5%BA%A6%E9%87%8F%E6%81%B6%E6%84%8F%E9%87%8D%E5%AE%9A%E5%90%91%E6%B4%BB%E5%8A%A8"><span class="toc-text">三十、从假新闻域中发现和度量恶意重定向活动</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%B8%89%E5%8D%81%E4%B8%80%E3%80%81Doc2Vec%E4%BB%A3%E7%A0%81%E5%AE%9E%E7%8E%B0"><span class="toc-text">三十一、Doc2Vec代码实现</span></a></li></ol>
    </div>
  </section>


  


</aside>



        
        
          <!--此文件用来存放一些不方便取值的变量-->
<!--思路大概是将值藏到重加载的区域内-->

<script>
  window.pdata={}
  pdata.ispage=true;
  pdata.postTitle="恶意URL检测(九)一些比较杂的文章";
  pdata.commentPath="";
  pdata.commentPlaceholder="";

  var l_header=document.getElementById("l_header");
  
  l_header.classList.add("show");
  
</script>

        
      </div>
      
  
  <footer class="footer clearfix">
    <br><br>
    
      
        <div class="aplayer-container">
          


        </div>
      
    
      
        <br>
        <div class="social-wrapper">
          
            
          
            
          
            
          
        </div>
      
    
      
        <div><p>Blog content follows the <a target="_blank" rel="noopener" href="https://creativecommons.org/licenses/by-nc-sa/4.0/deed.en">Attribution-NonCommercial-ShareAlike 4.0 International (CC BY-NC-SA 4.0) License</a></p>
</div>
      
    
      
        
          <div><p><span id="lc-sv">本站总访问量为 <span id='number'><i class="fas fa-circle-notch fa-spin fa-fw" aria-hidden="true"></i></span> 次</span> <span id="lc-uv">访客数为 <span id='number'><i class="fas fa-circle-notch fa-spin fa-fw" aria-hidden="true"></i></span> 人</span></p>
</div>
        
      
    
      
        Use
        <a href="https://github.com/volantis-x/hexo-theme-volantis/tree/4.2.0" target="_blank" class="codename">Volantis</a>
        as theme
      
    
      
        <div class='copyright'>
        <p><a href="/">Copyright © 2020-2021 1ss4k</a></p>

        </div>
      
    
  </footer>


      <a id="s-top" class="fas fa-arrow-up fa-fw" href='javascript:void(0)'></a>
    </div>
  </div>
  <div>
    <script>
window.volantis={};
window.volantis.loadcss=document.getElementById("loadcss");
/********************脚本懒加载函数********************************/
function loadScript(src, cb) {
var HEAD = document.getElementsByTagName('head')[0] || document.documentElement;
var script = document.createElement('script');
script.setAttribute('type','text/javascript');
if (cb) script.onload = cb;
script.setAttribute('src', src);
HEAD.appendChild(script);
}
//https://github.com/filamentgroup/loadCSS
var loadCSS = function( href, before, media, attributes ){
	var doc = window.document;
	var ss = doc.createElement( "link" );
	var ref;
	if( before ){
		ref = before;
	}
	else {
		var refs = ( doc.body || doc.getElementsByTagName( "head" )[ 0 ] ).childNodes;
		ref = refs[ refs.length - 1];
	}
	var sheets = doc.styleSheets;
	if( attributes ){
		for( var attributeName in attributes ){
			if( attributes.hasOwnProperty( attributeName ) ){
				ss.setAttribute( attributeName, attributes[attributeName] );
			}
		}
	}
	ss.rel = "stylesheet";
	ss.href = href;
	ss.media = "only x";
	function ready( cb ){
		if( doc.body ){
			return cb();
		}
		setTimeout(function(){
			ready( cb );
		});
	}
	ready( function(){
		ref.parentNode.insertBefore( ss, ( before ? ref : ref.nextSibling ) );
	});
	var onloadcssdefined = function( cb ){
		var resolvedHref = ss.href;
		var i = sheets.length;
		while( i-- ){
			if( sheets[ i ].href === resolvedHref ){
				return cb();
			}
		}
		setTimeout(function() {
			onloadcssdefined( cb );
		});
	};
	function loadCB(){
		if( ss.addEventListener ){
			ss.removeEventListener( "load", loadCB );
		}
		ss.media = media || "all";
	}
	if( ss.addEventListener ){
		ss.addEventListener( "load", loadCB);
	}
	ss.onloadcssdefined = onloadcssdefined;
	onloadcssdefined( loadCB );
	return ss;
};
</script>
<script>
  
  loadCSS("https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.14/css/all.min.css", window.volantis.loadcss);
  
  
  
  
</script>
<!-- required -->

<script src="https://cdn.jsdelivr.net/npm/jquery@3.5/dist/jquery.min.js"></script>

<script>
  function pjax_fancybox() {
    $(".md .gallery").find("img").each(function () { //渲染 fancybox
      var element = document.createElement("a"); // a 标签
      $(element).attr("class", "fancybox");
      $(element).attr("pjax-fancybox", "");  // 过滤 pjax
      $(element).attr("href", $(this).attr("src"));
      if ($(this).attr("data-original")) {
        $(element).attr("href", $(this).attr("data-original"));
      }
      $(element).attr("data-fancybox", "images");
      var caption = "";   // 描述信息
      if ($(this).attr('alt')) {  // 判断当前页面是否存在描述信息
        $(element).attr('data-caption', $(this).attr('alt'));
        caption = $(this).attr('alt');
      }
      var div = document.createElement("div");
      $(div).addClass("fancybox");
      $(this).wrap(div); // 最外层套 div ，其实主要作用还是 class 样式
      var span = document.createElement("span");
      $(span).addClass("image-caption");
      $(span).text(caption); // 加描述
      $(this).after(span);  // 再套一层描述
      $(this).wrap(element);  // 最后套 a 标签
    })
    $(".md .gallery").find("img").fancybox({
      selector: '[data-fancybox="images"]',
      hash: false,
      loop: false,
      closeClick: true,
      helpers: {
        overlay: {closeClick: true}
      },
      buttons: [
        "zoom",
        "close"
      ]
    });
  };
  function SCload_fancybox() {
    if ($(".md .gallery").find("img").length == 0) return;
    loadCSS("https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@3.5.7/dist/jquery.fancybox.min.css", document.getElementById("loadcss"));
    setTimeout(function() {
      loadScript('https://cdn.jsdelivr.net/gh/fancyapps/fancybox@3.5.7/dist/jquery.fancybox.min.js', pjax_fancybox)
    }, 1);
  };
  $(function () {
    SCload_fancybox();
  });
</script>


<!-- internal -->







  <script defer src="https://cdn.jsdelivr.net/npm/vanilla-lazyload@17.1.0/dist/lazyload.min.js"></script>
<script>
  // https://www.npmjs.com/package/vanilla-lazyload
  // Set the options globally
  // to make LazyLoad self-initialize
  window.lazyLoadOptions = {
    elements_selector: ".lazyload",
    threshold: 0
  };
  // Listen to the initialization event
  // and get the instance of LazyLoad
  window.addEventListener(
    "LazyLoad::Initialized",
    function (event) {
      window.lazyLoadInstance = event.detail.instance;
    },
    false
  );
  document.addEventListener('DOMContentLoaded', function () {
    lazyLoadInstance.update();
  });
  document.addEventListener('pjax:complete', function () {
    lazyLoadInstance.update();
  });
</script>




  
  
    <script>
      window.FPConfig = {
        delay: 0,
        ignoreKeywords: [],
        maxRPS: 5,
        hoverDelay: 25
      };
    </script>
    <script defer src="https://cdn.jsdelivr.net/gh/gijo-varghese/flying-pages@2.1.2/flying-pages.min.js"></script>
  










  
  
<script src="/js/valine.js"></script>


<script>
  var GUEST_INFO = ['nick', 'mail', 'link'];
  var meta = 'nick,mail,link'.split(',').filter(function (item) {
    return GUEST_INFO.indexOf(item) > -1
  });
  var REQUIRED_FIELDS = ['nick', 'mail', 'link'];
  var requiredFields = 'nick,mail'.split(',').filter(function (item) {
    return REQUIRED_FIELDS.indexOf(item) > -1
  });

  function emoji(path, idx, ext) {
    return path + "/" + path + "-" + idx + "." + ext;
  }

  var emojiMaps = {};
  for (var i = 1; i <= 54; i++) {
    emojiMaps['tieba-' + i] = emoji('tieba', i, 'png');
  }
  for (var i = 1; i <= 101; i++) {
    emojiMaps['qq-' + i] = emoji('qq', i, 'gif');
  }
  for (var i = 1; i <= 116; i++) {
    emojiMaps['aru-' + i] = emoji('aru', i, 'gif');
  }
  for (var i = 1; i <= 125; i++) {
    emojiMaps['twemoji-' + i] = emoji('twemoji', i, 'png');
  }
  for (var i = 1; i <= 4; i++) {
    emojiMaps['weibo-' + i] = emoji('weibo', i, 'png');
  }

  function pjax_valine() {
    if(!document.querySelectorAll("#valine_container")[0])return;

    let pagePlaceholder = pdata.commentPlaceholder || "快来评论吧~";

    let path = pdata.commentPath;
    if (path.length == 0) {
      let defaultPath = '';
      path = defaultPath || decodeURI(window.location.pathname);
    }

    var valine = new Valine();
    valine.init({
      el: '#valine_container',
      meta: meta,
      placeholder: pagePlaceholder,
      path: path,
      appId: "",
      appKey: "",
      pageSize: '10',
      avatar: 'robohash',
      lang: 'zh-cn',
      visitor: 'true',
      highlight: 'true',
      mathJax: 'false',
      enableQQ: 'true',
      recordIP: 'false',
      requiredFields: requiredFields,
      emojiCDN: 'https://cdn.jsdelivr.net/gh/volantis-x/cdn-emoji/valine/',
      emojiMaps: emojiMaps
    })
  }

  $(function () {
    pjax_valine();
  });
</script>





  
<script src="/js/app.js"></script>



<!-- optional -->

  <script>
const SearchServiceimagePath="https://cdn.jsdelivr.net/gh/volantis-x/cdn-volantis@master/img/";
const ROOT =  ("/" || "/").endsWith('/') ? ("/" || "/") : ("//" || "/" );

$('.input.u-search-input').one('focus',function(){
	
		loadScript('/js/search/hexo.js',setSearchService);
	
})

function listenSearch(){
  
    customSearch = new HexoSearch({
      imagePath: SearchServiceimagePath
    });
  
}
function setSearchService() {
	listenSearch();
	
}
</script>











  <script defer>

  const LCCounter = {
    app_id: 'u9j57bwJod4EDmXWdxrwuqQT-MdYXbMMI',
    app_key: 'jfHtEKVE24j0IVCGHbvuFClp',
    custom_api_server: '',

    // 查询存储的记录
    getRecord(Counter, url, title) {
      return new Promise(function (resolve, reject) {
        Counter('get', '/classes/Counter?where=' + encodeURIComponent(JSON.stringify({url})))
          .then(resp => resp.json())
          .then(({results, code, error}) => {
            if (code === 401) {
              throw error;
            }
            if (results && results.length > 0) {
              var record = results[0];
              resolve(record);
            } else {
              Counter('post', '/classes/Counter', {url, title: title, times: 0})
                .then(resp => resp.json())
                .then((record, error) => {
                  if (error) {
                    throw error;
                  }
                  resolve(record);
                }).catch(error => {
                console.error('Failed to create', error);
                reject(error);
              });
            }
          }).catch((error) => {
          console.error('LeanCloud Counter Error:', error);
          reject(error);
        });
      })
    },

    // 发起自增请求
    increment(Counter, incrArr) {
      return new Promise(function (resolve, reject) {
        Counter('post', '/batch', {
          "requests": incrArr
        }).then((res) => {
          res = res.json();
          if (res.error) {
            throw res.error;
          }
          resolve(res);
        }).catch((error) => {
          console.error('Failed to save visitor count', error);
          reject(error);
        });
      });
    },

    // 构建自增请求体
    buildIncrement(objectId) {
      return {
        "method": "PUT",
        "path": `/1.1/classes/Counter/${ objectId }`,
        "body": {
          "times": {
            '__op': 'Increment',
            'amount': 1
          }
        }
      }
    },

    // 校验是否为有效的 UV
    validUV() {
      var key = 'LeanCloudUVTimestamp';
      var flag = localStorage.getItem(key);
      if (flag) {
        // 距离标记小于 24 小时则不计为 UV
        if (new Date().getTime() - parseInt(flag) <= 86400000) {
          return false;
        }
      }
      localStorage.setItem(key, new Date().getTime().toString());
      return true;
    },

    addCount(Counter) {
      var enableIncr = '' === 'true' && window.location.hostname !== 'localhost';
      enableIncr = true;
      var getterArr = [];
      var incrArr = [];
      // 请求 PV 并自增
      var pvCtn = document.querySelector('#lc-sv');
      if (pvCtn || enableIncr) {
        var pvGetter = this.getRecord(Counter, 'http://example.com' + '/#lc-sv', 'Visits').then((record) => {
          incrArr.push(this.buildIncrement(record.objectId))
          var eles = document.querySelectorAll('#lc-sv #number');
          if (eles.length > 0) {
            eles.forEach((el,index,array)=>{
              el.innerText = record.times + 1;
              if (pvCtn) {
                pvCtn.style.display = 'inline';
              }
            })
          }
        });
        getterArr.push(pvGetter);
      }

      // 请求 UV 并自增
      var uvCtn = document.querySelector('#lc-uv');
      if (uvCtn || enableIncr) {
        var uvGetter = this.getRecord(Counter, 'http://example.com' + '/#lc-uv', 'Visitors').then((record) => {
          var vuv = this.validUV();
          vuv && incrArr.push(this.buildIncrement(record.objectId))
          var eles = document.querySelectorAll('#lc-uv #number');
          if (eles.length > 0) {
            eles.forEach((el,index,array)=>{
              el.innerText = record.times + (vuv ? 1 : 0);
              if (uvCtn) {
                uvCtn.style.display = 'inline';
              }
            })
          }
        });
        getterArr.push(uvGetter);
      }

      // 请求文章的浏览数，如果是当前页面就自增
      var allPV = document.querySelectorAll('#lc-pv');
      if (allPV.length > 0 || enableIncr) {
        for (i = 0; i < allPV.length; i++) {
          let pv = allPV[i];
          let title = pv.getAttribute('data-title');
          var url = 'http://example.com' + pv.getAttribute('data-path');
          if (url) {
            var viewGetter = this.getRecord(Counter, url, title).then((record) => {
              // 是当前页面就自增
              let curPath = window.location.pathname;
              if (curPath.includes('index.html')) {
                curPath = curPath.substring(0, curPath.lastIndexOf('index.html'));
              }
              if (pv.getAttribute('data-path') == curPath) {
                incrArr.push(this.buildIncrement(record.objectId));
              }
              if (pv) {
                var ele = pv.querySelector('#lc-pv #number');
                if (ele) {
                  if (pv.getAttribute('data-path') == curPath) {
                    ele.innerText = (record.times || 0) + 1;
                  } else {
                    ele.innerText = record.times || 0;
                  }
                  pv.style.display = 'inline';
                }
              }
            });
            getterArr.push(viewGetter);
          }
        }
      }

      // 如果启动计数自增，批量发起自增请求
      if (enableIncr) {
        Promise.all(getterArr).then(() => {
          incrArr.length > 0 && this.increment(Counter, incrArr);
        })
      }

    },


    fetchData(api_server) {
      var Counter = (method, url, data) => {
        return fetch(`${ api_server }/1.1${ url }`, {
          method,
          headers: {
            'X-LC-Id': this.app_id,
            'X-LC-Key': this.app_key,
            'Content-Type': 'application/json',
          },
          body: JSON.stringify(data)
        });
      };
      this.addCount(Counter);
    },

    refreshCounter() {
      var api_server = this.app_id.slice(-9) !== '-MdYXbMMI' ? this.custom_api_server : `https://${ this.app_id.slice(0, 8).toLowerCase() }.api.lncldglobal.com`;
      if (api_server) {
        this.fetchData(api_server);
      } else {
        fetch('https://app-router.leancloud.cn/2/route?appId=' + this.app_id)
          .then(resp => resp.json())
          .then(({api_server}) => {
            this.fetchData('https://' + api_server);
          });
      }
    }

  };

  LCCounter.refreshCounter();

  document.addEventListener('pjax:complete', function () {
    LCCounter.refreshCounter();
  });
</script>








<script>
function listennSidebarTOC() {
  const navItems = document.querySelectorAll(".toc li");
  if (!navItems.length) return;
  const sections = [...navItems].map((element) => {
    const link = element.querySelector(".toc-link");
    const target = document.getElementById(
      decodeURI(link.getAttribute("href")).replace("#", "")
    );
    link.addEventListener("click", (event) => {
      event.preventDefault();
      window.scrollTo({
		top: target.offsetTop + 100,
		
		behavior: "smooth"
		
	  });
    });
    return target;
  });

  function activateNavByIndex(target) {
    if (target.classList.contains("active-current")) return;

    document.querySelectorAll(".toc .active").forEach((element) => {
      element.classList.remove("active", "active-current");
    });
    target.classList.add("active", "active-current");
    let parent = target.parentNode;
    while (!parent.matches(".toc")) {
      if (parent.matches("li")) parent.classList.add("active");
      parent = parent.parentNode;
    }
  }

  function findIndex(entries) {
    let index = 0;
    let entry = entries[index];
    if (entry.boundingClientRect.top > 0) {
      index = sections.indexOf(entry.target);
      return index === 0 ? 0 : index - 1;
    }
    for (; index < entries.length; index++) {
      if (entries[index].boundingClientRect.top <= 0) {
        entry = entries[index];
      } else {
        return sections.indexOf(entry.target);
      }
    }
    return sections.indexOf(entry.target);
  }

  function createIntersectionObserver(marginTop) {
    marginTop = Math.floor(marginTop + 10000);
    let intersectionObserver = new IntersectionObserver(
      (entries, observe) => {
        let scrollHeight = document.documentElement.scrollHeight + 100;
        if (scrollHeight > marginTop) {
          observe.disconnect();
          createIntersectionObserver(scrollHeight);
          return;
        }
        let index = findIndex(entries);
        activateNavByIndex(navItems[index]);
      },
      {
        rootMargin: marginTop + "px 0px -100% 0px",
        threshold: 0,
      }
    );
    sections.forEach((element) => {
      element && intersectionObserver.observe(element);
    });
  }
  createIntersectionObserver(document.documentElement.scrollHeight);
}

document.addEventListener("DOMContentLoaded", listennSidebarTOC);
document.addEventListener("pjax:success", listennSidebarTOC);
</script>

<!-- more -->




    
      


<script src="https://cdn.jsdelivr.net/npm/pjax@0.2.8/pjax.min.js"></script>


<script>
    var pjax;
    document.addEventListener('DOMContentLoaded', function () {
      pjax = new Pjax({
        elements: 'a[href]:not([href^="#"]):not([href="javascript:void(0)"]):not([pjax-fancybox])',
        selectors: [
          "title",
          "#l_cover",
          "#pjax-container",
          "#pjax-header-nav-list"
        ],
        cacheBust: false,   // url 地址追加时间戳，用以避免浏览器缓存
        timeout: 5000
      });
    });

    document.addEventListener('pjax:send', function (e) {
      //window.stop(); // 相当于点击了浏览器的停止按钮

      try {
        var currentUrl = window.location.pathname;
        var targetUrl = e.triggerElement.href;
        var banUrl = [""];
        if (banUrl[0] != "") {
          banUrl.forEach(item => {
            if(currentUrl.indexOf(item) != -1 || targetUrl.indexOf(item) != -1) {
              window.location.href = targetUrl;
            }
          });
        }
      } catch (error) {}

      window.subData = null; // 移除标题（用于一二级导航栏切换处）
      if (typeof $.fancybox != "undefined") {
        $.fancybox.close();    // 关闭弹窗
      }
      volantis.$switcher.removeClass('active'); // 关闭移动端激活的搜索框
      volantis.$header.removeClass('z_search-open'); // 关闭移动端激活的搜索框
      volantis.$wrapper.removeClass('sub'); // 跳转页面时关闭二级导航

      // 解绑事件 避免重复监听
      volantis.$topBtn.unbind('click');
      $('.menu a').unbind('click');
      $(window).unbind('resize');
      $(window).unbind('scroll');
      $(document).unbind('scroll');
      $(document).unbind('click');
      $('body').unbind('click');
	  
    });

    document.addEventListener('pjax:complete', function () {
      // 关于百度统计对 SPA 页面的处理：
      // 方案一：百度统计>管理>单页应用设置中，打开开启按钮即可对SPA进行统计。 https://tongji.baidu.com/web/help/article?id=324
      // 方案二：取消注释下列代码。 https://tongji.baidu.com/web/help/article?id=235
       

      // 关于谷歌统计对 SPA 页面的处理：
      // 当应用以动态方式加载内容并更新地址栏中的网址时，也应该更新通过 gtag.js 存储的网页网址。
      // https://developers.google.cn/analytics/devguides/collection/gtagjs/single-page-applications?hl=zh-cn
      
	 

      $('.nav-main').find('.list-v').not('.menu-phone').removeAttr("style",""); // 移除小尾巴的移除
      $('.menu-phone.list-v').removeAttr("style",""); // 移除小尾巴的移除
      $('script[data-pjax], .pjax-reload script').each(function () {
        $(this).parent().append($(this).remove());
      });
      try{
          if (typeof $.fancybox == "undefined") {
            SCload_fancybox();
          } else {
            pjax_fancybox();
          }
        
        
        
        
        
        
          pjax_valine();
        
        
        
        
        
      } catch (e) {
        console.log(e);
      }
	  
    });

    document.addEventListener('pjax:error', function (e) {
	  
      window.location.href = e.triggerElement.href;
    });
</script>

    
  </div>
</body>
</html>
